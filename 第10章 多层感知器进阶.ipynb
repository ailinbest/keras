{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. json序列化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "from keras.models import model_from_yaml\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#由一维转化为二维了\n",
    "y = to_categorical(y,num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 三层，两个隐藏层分别是4，6个神经元，输出层有3个units，并且激活函数为softmax\n",
    "- loss采用的是分类交叉熵,categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建模型\n",
    "def create_model(optimizer='rmsprop',init='glorot_uniform'):\n",
    "    model = Sequential()\n",
    "    # 输入值是4维的，(*,4)\n",
    "    model.add(Dense(units=4,activation='relu',kernel_initializer=init,input_shape=(4,)))\n",
    "    model.add(Dense(units=6,activation='relu',kernel_initializer=init))\n",
    "    model.add(Dense(units=3,activation='softmax',kernel_initializer=init))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "112/112 [==============================] - 0s - loss: 1.1104 - acc: 0.6250     \n",
      "Epoch 2/200\n",
      "112/112 [==============================] - 0s - loss: 0.9790 - acc: 0.6786     \n",
      "Epoch 3/200\n",
      "112/112 [==============================] - 0s - loss: 0.8938 - acc: 0.6786     \n",
      "Epoch 4/200\n",
      "112/112 [==============================] - 0s - loss: 0.8317 - acc: 0.6786     \n",
      "Epoch 5/200\n",
      "112/112 [==============================] - 0s - loss: 0.7796 - acc: 0.6786     \n",
      "Epoch 6/200\n",
      "112/112 [==============================] - 0s - loss: 0.7331 - acc: 0.6786     \n",
      "Epoch 7/200\n",
      "112/112 [==============================] - 0s - loss: 0.6949 - acc: 0.6786     \n",
      "Epoch 8/200\n",
      "112/112 [==============================] - 0s - loss: 0.6694 - acc: 0.6786     \n",
      "Epoch 9/200\n",
      "112/112 [==============================] - 0s - loss: 0.6467 - acc: 0.6786     \n",
      "Epoch 10/200\n",
      "112/112 [==============================] - 0s - loss: 0.6271 - acc: 0.6786     \n",
      "Epoch 11/200\n",
      "112/112 [==============================] - 0s - loss: 0.6086 - acc: 0.6786     \n",
      "Epoch 12/200\n",
      "112/112 [==============================] - 0s - loss: 0.5917 - acc: 0.6786     \n",
      "Epoch 13/200\n",
      "112/112 [==============================] - 0s - loss: 0.5756 - acc: 0.6786     \n",
      "Epoch 14/200\n",
      "112/112 [==============================] - 0s - loss: 0.5608 - acc: 0.6786     \n",
      "Epoch 15/200\n",
      "112/112 [==============================] - 0s - loss: 0.5458 - acc: 0.6786     \n",
      "Epoch 16/200\n",
      "112/112 [==============================] - 0s - loss: 0.5314 - acc: 0.6786     \n",
      "Epoch 17/200\n",
      "112/112 [==============================] - 0s - loss: 0.5191 - acc: 0.6786     \n",
      "Epoch 18/200\n",
      "112/112 [==============================] - 0s - loss: 0.5085 - acc: 0.6786     \n",
      "Epoch 19/200\n",
      "112/112 [==============================] - 0s - loss: 0.4972 - acc: 0.6786     \n",
      "Epoch 20/200\n",
      "112/112 [==============================] - 0s - loss: 0.4868 - acc: 0.6786     \n",
      "Epoch 21/200\n",
      "112/112 [==============================] - 0s - loss: 0.4773 - acc: 0.6786     \n",
      "Epoch 22/200\n",
      "112/112 [==============================] - 0s - loss: 0.4665 - acc: 0.6786     \n",
      "Epoch 23/200\n",
      "112/112 [==============================] - 0s - loss: 0.4576 - acc: 0.6875     \n",
      "Epoch 24/200\n",
      "112/112 [==============================] - 0s - loss: 0.4488 - acc: 0.6786     \n",
      "Epoch 25/200\n",
      "112/112 [==============================] - 0s - loss: 0.4395 - acc: 0.7143     \n",
      "Epoch 26/200\n",
      "112/112 [==============================] - 0s - loss: 0.4320 - acc: 0.7143     \n",
      "Epoch 27/200\n",
      "112/112 [==============================] - 0s - loss: 0.4234 - acc: 0.7054     \n",
      "Epoch 28/200\n",
      "112/112 [==============================] - 0s - loss: 0.4166 - acc: 0.7411     \n",
      "Epoch 29/200\n",
      "112/112 [==============================] - 0s - loss: 0.4092 - acc: 0.7946     \n",
      "Epoch 30/200\n",
      "112/112 [==============================] - 0s - loss: 0.4018 - acc: 0.8214     \n",
      "Epoch 31/200\n",
      "112/112 [==============================] - 0s - loss: 0.3961 - acc: 0.8304     \n",
      "Epoch 32/200\n",
      "112/112 [==============================] - 0s - loss: 0.3904 - acc: 0.8304     \n",
      "Epoch 33/200\n",
      "112/112 [==============================] - 0s - loss: 0.3841 - acc: 0.8482     \n",
      "Epoch 34/200\n",
      "112/112 [==============================] - 0s - loss: 0.3799 - acc: 0.8482     \n",
      "Epoch 35/200\n",
      "112/112 [==============================] - 0s - loss: 0.3753 - acc: 0.8571     \n",
      "Epoch 36/200\n",
      "112/112 [==============================] - 0s - loss: 0.3709 - acc: 0.8571     \n",
      "Epoch 37/200\n",
      "112/112 [==============================] - 0s - loss: 0.3651 - acc: 0.8661     \n",
      "Epoch 38/200\n",
      "112/112 [==============================] - 0s - loss: 0.3633 - acc: 0.9018     \n",
      "Epoch 39/200\n",
      "112/112 [==============================] - 0s - loss: 0.3588 - acc: 0.8929     \n",
      "Epoch 40/200\n",
      "112/112 [==============================] - 0s - loss: 0.3551 - acc: 0.8482     \n",
      "Epoch 41/200\n",
      "112/112 [==============================] - 0s - loss: 0.3513 - acc: 0.9196     \n",
      "Epoch 42/200\n",
      "112/112 [==============================] - 0s - loss: 0.3483 - acc: 0.8839     \n",
      "Epoch 43/200\n",
      "112/112 [==============================] - 0s - loss: 0.3451 - acc: 0.8929     \n",
      "Epoch 44/200\n",
      "112/112 [==============================] - 0s - loss: 0.3399 - acc: 0.8839     \n",
      "Epoch 45/200\n",
      "112/112 [==============================] - 0s - loss: 0.3372 - acc: 0.9286     \n",
      "Epoch 46/200\n",
      "112/112 [==============================] - 0s - loss: 0.3338 - acc: 0.8929     \n",
      "Epoch 47/200\n",
      "112/112 [==============================] - 0s - loss: 0.3306 - acc: 0.9018     \n",
      "Epoch 48/200\n",
      "112/112 [==============================] - 0s - loss: 0.3263 - acc: 0.9286     \n",
      "Epoch 49/200\n",
      "112/112 [==============================] - 0s - loss: 0.3224 - acc: 0.8929     \n",
      "Epoch 50/200\n",
      "112/112 [==============================] - 0s - loss: 0.3203 - acc: 0.9286     \n",
      "Epoch 51/200\n",
      "112/112 [==============================] - 0s - loss: 0.3174 - acc: 0.9464     \n",
      "Epoch 52/200\n",
      "112/112 [==============================] - 0s - loss: 0.3153 - acc: 0.9107     \n",
      "Epoch 53/200\n",
      "112/112 [==============================] - 0s - loss: 0.3109 - acc: 0.9554     \n",
      "Epoch 54/200\n",
      "112/112 [==============================] - 0s - loss: 0.3095 - acc: 0.9375     \n",
      "Epoch 55/200\n",
      "112/112 [==============================] - 0s - loss: 0.3057 - acc: 0.9375     \n",
      "Epoch 56/200\n",
      "112/112 [==============================] - 0s - loss: 0.3021 - acc: 0.9196     \n",
      "Epoch 57/200\n",
      "112/112 [==============================] - 0s - loss: 0.2993 - acc: 0.9375     \n",
      "Epoch 58/200\n",
      "112/112 [==============================] - 0s - loss: 0.2975 - acc: 0.9464     \n",
      "Epoch 59/200\n",
      "112/112 [==============================] - 0s - loss: 0.2924 - acc: 0.9464     \n",
      "Epoch 60/200\n",
      "112/112 [==============================] - 0s - loss: 0.2898 - acc: 0.9286     \n",
      "Epoch 61/200\n",
      "112/112 [==============================] - 0s - loss: 0.2861 - acc: 0.9554     \n",
      "Epoch 62/200\n",
      "112/112 [==============================] - 0s - loss: 0.2843 - acc: 0.9375     \n",
      "Epoch 63/200\n",
      "112/112 [==============================] - 0s - loss: 0.2806 - acc: 0.9375     \n",
      "Epoch 64/200\n",
      "112/112 [==============================] - 0s - loss: 0.2779 - acc: 0.9554     \n",
      "Epoch 65/200\n",
      "112/112 [==============================] - 0s - loss: 0.2753 - acc: 0.9375     \n",
      "Epoch 66/200\n",
      "112/112 [==============================] - 0s - loss: 0.2720 - acc: 0.9464     \n",
      "Epoch 67/200\n",
      "112/112 [==============================] - 0s - loss: 0.2706 - acc: 0.9554     \n",
      "Epoch 68/200\n",
      "112/112 [==============================] - 0s - loss: 0.2688 - acc: 0.9464     \n",
      "Epoch 69/200\n",
      "112/112 [==============================] - 0s - loss: 0.2634 - acc: 0.9643     \n",
      "Epoch 70/200\n",
      "112/112 [==============================] - 0s - loss: 0.2628 - acc: 0.9464     \n",
      "Epoch 71/200\n",
      "112/112 [==============================] - 0s - loss: 0.2587 - acc: 0.9554     \n",
      "Epoch 72/200\n",
      "112/112 [==============================] - 0s - loss: 0.2553 - acc: 0.9643     \n",
      "Epoch 73/200\n",
      "112/112 [==============================] - 0s - loss: 0.2548 - acc: 0.9643     \n",
      "Epoch 74/200\n",
      "112/112 [==============================] - 0s - loss: 0.2512 - acc: 0.9375     \n",
      "Epoch 75/200\n",
      "112/112 [==============================] - 0s - loss: 0.2469 - acc: 0.9554     \n",
      "Epoch 76/200\n",
      "112/112 [==============================] - 0s - loss: 0.2454 - acc: 0.9464     \n",
      "Epoch 77/200\n",
      "112/112 [==============================] - 0s - loss: 0.2428 - acc: 0.9643     \n",
      "Epoch 78/200\n",
      "112/112 [==============================] - 0s - loss: 0.2384 - acc: 0.9643     \n",
      "Epoch 79/200\n",
      "112/112 [==============================] - 0s - loss: 0.2364 - acc: 0.9732     \n",
      "Epoch 80/200\n",
      "112/112 [==============================] - 0s - loss: 0.2338 - acc: 0.9732     \n",
      "Epoch 81/200\n",
      "112/112 [==============================] - 0s - loss: 0.2323 - acc: 0.9643     \n",
      "Epoch 82/200\n",
      "112/112 [==============================] - 0s - loss: 0.2289 - acc: 0.9732     \n",
      "Epoch 83/200\n",
      "112/112 [==============================] - 0s - loss: 0.2265 - acc: 0.9732     \n",
      "Epoch 84/200\n",
      "112/112 [==============================] - 0s - loss: 0.2257 - acc: 0.9643     \n",
      "Epoch 85/200\n",
      "112/112 [==============================] - 0s - loss: 0.2208 - acc: 0.9643     \n",
      "Epoch 86/200\n",
      "112/112 [==============================] - 0s - loss: 0.2200 - acc: 0.9643     \n",
      "Epoch 87/200\n",
      "112/112 [==============================] - 0s - loss: 0.2188 - acc: 0.9732     \n",
      "Epoch 88/200\n",
      "112/112 [==============================] - 0s - loss: 0.2157 - acc: 0.9732     \n",
      "Epoch 89/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s - loss: 0.2127 - acc: 0.9732     \n",
      "Epoch 90/200\n",
      "112/112 [==============================] - 0s - loss: 0.2101 - acc: 0.9732     \n",
      "Epoch 91/200\n",
      "112/112 [==============================] - 0s - loss: 0.2091 - acc: 0.9732     \n",
      "Epoch 92/200\n",
      "112/112 [==============================] - 0s - loss: 0.2025 - acc: 0.9732     \n",
      "Epoch 93/200\n",
      "112/112 [==============================] - 0s - loss: 0.2049 - acc: 0.9732     \n",
      "Epoch 94/200\n",
      "112/112 [==============================] - 0s - loss: 0.2012 - acc: 0.9732     \n",
      "Epoch 95/200\n",
      "112/112 [==============================] - 0s - loss: 0.2001 - acc: 0.9732     \n",
      "Epoch 96/200\n",
      "112/112 [==============================] - 0s - loss: 0.1983 - acc: 0.9732     \n",
      "Epoch 97/200\n",
      "112/112 [==============================] - 0s - loss: 0.1937 - acc: 0.9732     \n",
      "Epoch 98/200\n",
      "112/112 [==============================] - 0s - loss: 0.1941 - acc: 0.9732     \n",
      "Epoch 99/200\n",
      "112/112 [==============================] - 0s - loss: 0.1912 - acc: 0.9732     \n",
      "Epoch 100/200\n",
      "112/112 [==============================] - 0s - loss: 0.1882 - acc: 0.9732     \n",
      "Epoch 101/200\n",
      "112/112 [==============================] - 0s - loss: 0.1886 - acc: 0.9732     \n",
      "Epoch 102/200\n",
      "112/112 [==============================] - 0s - loss: 0.1865 - acc: 0.9732     \n",
      "Epoch 103/200\n",
      "112/112 [==============================] - 0s - loss: 0.1853 - acc: 0.9732     \n",
      "Epoch 104/200\n",
      "112/112 [==============================] - 0s - loss: 0.1824 - acc: 0.9732     \n",
      "Epoch 105/200\n",
      "112/112 [==============================] - 0s - loss: 0.1803 - acc: 0.9732     \n",
      "Epoch 106/200\n",
      "112/112 [==============================] - 0s - loss: 0.1765 - acc: 0.9732     \n",
      "Epoch 107/200\n",
      "112/112 [==============================] - 0s - loss: 0.1763 - acc: 0.9732     \n",
      "Epoch 108/200\n",
      "112/112 [==============================] - 0s - loss: 0.1743 - acc: 0.9643     \n",
      "Epoch 109/200\n",
      "112/112 [==============================] - 0s - loss: 0.1745 - acc: 0.9732     \n",
      "Epoch 110/200\n",
      "112/112 [==============================] - 0s - loss: 0.1726 - acc: 0.9732     \n",
      "Epoch 111/200\n",
      "112/112 [==============================] - 0s - loss: 0.1720 - acc: 0.9732     \n",
      "Epoch 112/200\n",
      "112/112 [==============================] - 0s - loss: 0.1682 - acc: 0.9732     \n",
      "Epoch 113/200\n",
      "112/112 [==============================] - 0s - loss: 0.1672 - acc: 0.9732     \n",
      "Epoch 114/200\n",
      "112/112 [==============================] - 0s - loss: 0.1650 - acc: 0.9732     \n",
      "Epoch 115/200\n",
      "112/112 [==============================] - 0s - loss: 0.1641 - acc: 0.9732     \n",
      "Epoch 116/200\n",
      "112/112 [==============================] - 0s - loss: 0.1620 - acc: 0.9732     \n",
      "Epoch 117/200\n",
      "112/112 [==============================] - 0s - loss: 0.1609 - acc: 0.9732     \n",
      "Epoch 118/200\n",
      "112/112 [==============================] - 0s - loss: 0.1585 - acc: 0.9732     \n",
      "Epoch 119/200\n",
      "112/112 [==============================] - 0s - loss: 0.1582 - acc: 0.9732     \n",
      "Epoch 120/200\n",
      "112/112 [==============================] - 0s - loss: 0.1565 - acc: 0.9732     \n",
      "Epoch 121/200\n",
      "112/112 [==============================] - 0s - loss: 0.1541 - acc: 0.9732     \n",
      "Epoch 122/200\n",
      "112/112 [==============================] - 0s - loss: 0.1520 - acc: 0.9732     \n",
      "Epoch 123/200\n",
      "112/112 [==============================] - 0s - loss: 0.1516 - acc: 0.9732     \n",
      "Epoch 124/200\n",
      "112/112 [==============================] - 0s - loss: 0.1512 - acc: 0.9732     \n",
      "Epoch 125/200\n",
      "112/112 [==============================] - 0s - loss: 0.1472 - acc: 0.9732     \n",
      "Epoch 126/200\n",
      "112/112 [==============================] - 0s - loss: 0.1481 - acc: 0.9732     \n",
      "Epoch 127/200\n",
      "112/112 [==============================] - 0s - loss: 0.1497 - acc: 0.9732     \n",
      "Epoch 128/200\n",
      "112/112 [==============================] - 0s - loss: 0.1459 - acc: 0.9732     \n",
      "Epoch 129/200\n",
      "112/112 [==============================] - 0s - loss: 0.1435 - acc: 0.9732     \n",
      "Epoch 130/200\n",
      "112/112 [==============================] - 0s - loss: 0.1448 - acc: 0.9732     \n",
      "Epoch 131/200\n",
      "112/112 [==============================] - 0s - loss: 0.1398 - acc: 0.9643     \n",
      "Epoch 132/200\n",
      "112/112 [==============================] - 0s - loss: 0.1436 - acc: 0.9732     \n",
      "Epoch 133/200\n",
      "112/112 [==============================] - 0s - loss: 0.1383 - acc: 0.9732     \n",
      "Epoch 134/200\n",
      "112/112 [==============================] - 0s - loss: 0.1385 - acc: 0.9732     \n",
      "Epoch 135/200\n",
      "112/112 [==============================] - 0s - loss: 0.1362 - acc: 0.9732     \n",
      "Epoch 136/200\n",
      "112/112 [==============================] - 0s - loss: 0.1361 - acc: 0.9732     \n",
      "Epoch 137/200\n",
      "112/112 [==============================] - 0s - loss: 0.1333 - acc: 0.9643     \n",
      "Epoch 138/200\n",
      "112/112 [==============================] - 0s - loss: 0.1342 - acc: 0.9732     \n",
      "Epoch 139/200\n",
      "112/112 [==============================] - 0s - loss: 0.1326 - acc: 0.9732     \n",
      "Epoch 140/200\n",
      "112/112 [==============================] - 0s - loss: 0.1311 - acc: 0.9732     \n",
      "Epoch 141/200\n",
      "112/112 [==============================] - 0s - loss: 0.1305 - acc: 0.9732     \n",
      "Epoch 142/200\n",
      "112/112 [==============================] - 0s - loss: 0.1293 - acc: 0.9732     \n",
      "Epoch 143/200\n",
      "112/112 [==============================] - 0s - loss: 0.1279 - acc: 0.9732     \n",
      "Epoch 144/200\n",
      "112/112 [==============================] - 0s - loss: 0.1268 - acc: 0.9732     \n",
      "Epoch 145/200\n",
      "112/112 [==============================] - 0s - loss: 0.1267 - acc: 0.9732     \n",
      "Epoch 146/200\n",
      "112/112 [==============================] - 0s - loss: 0.1258 - acc: 0.9732     \n",
      "Epoch 147/200\n",
      "112/112 [==============================] - 0s - loss: 0.1253 - acc: 0.9732     \n",
      "Epoch 148/200\n",
      "112/112 [==============================] - 0s - loss: 0.1246 - acc: 0.9732     \n",
      "Epoch 149/200\n",
      "112/112 [==============================] - 0s - loss: 0.1225 - acc: 0.9732     \n",
      "Epoch 150/200\n",
      "112/112 [==============================] - 0s - loss: 0.1219 - acc: 0.9732     \n",
      "Epoch 151/200\n",
      "112/112 [==============================] - 0s - loss: 0.1196 - acc: 0.9732     \n",
      "Epoch 152/200\n",
      "112/112 [==============================] - 0s - loss: 0.1216 - acc: 0.9732     \n",
      "Epoch 153/200\n",
      "112/112 [==============================] - 0s - loss: 0.1190 - acc: 0.9732     \n",
      "Epoch 154/200\n",
      "112/112 [==============================] - 0s - loss: 0.1184 - acc: 0.9732     \n",
      "Epoch 155/200\n",
      "112/112 [==============================] - 0s - loss: 0.1197 - acc: 0.9732     \n",
      "Epoch 156/200\n",
      "112/112 [==============================] - 0s - loss: 0.1168 - acc: 0.9732     \n",
      "Epoch 157/200\n",
      "112/112 [==============================] - 0s - loss: 0.1173 - acc: 0.9732     \n",
      "Epoch 158/200\n",
      "112/112 [==============================] - 0s - loss: 0.1143 - acc: 0.9732     \n",
      "Epoch 159/200\n",
      "112/112 [==============================] - 0s - loss: 0.1167 - acc: 0.9732     \n",
      "Epoch 160/200\n",
      "112/112 [==============================] - 0s - loss: 0.1145 - acc: 0.9732     \n",
      "Epoch 161/200\n",
      "112/112 [==============================] - 0s - loss: 0.1147 - acc: 0.9732     \n",
      "Epoch 162/200\n",
      "112/112 [==============================] - 0s - loss: 0.1149 - acc: 0.9732         \n",
      "Epoch 163/200\n",
      "112/112 [==============================] - 0s - loss: 0.1126 - acc: 0.9732     \n",
      "Epoch 164/200\n",
      "112/112 [==============================] - 0s - loss: 0.1121 - acc: 0.9732     \n",
      "Epoch 165/200\n",
      "112/112 [==============================] - 0s - loss: 0.1102 - acc: 0.9732     \n",
      "Epoch 166/200\n",
      "112/112 [==============================] - 0s - loss: 0.1113 - acc: 0.9732     \n",
      "Epoch 167/200\n",
      "112/112 [==============================] - 0s - loss: 0.1093 - acc: 0.9732     \n",
      "Epoch 168/200\n",
      "112/112 [==============================] - 0s - loss: 0.1093 - acc: 0.9732     \n",
      "Epoch 169/200\n",
      "112/112 [==============================] - 0s - loss: 0.1105 - acc: 0.9732     \n",
      "Epoch 170/200\n",
      "112/112 [==============================] - 0s - loss: 0.1092 - acc: 0.9732     \n",
      "Epoch 171/200\n",
      "112/112 [==============================] - 0s - loss: 0.1074 - acc: 0.9732     \n",
      "Epoch 172/200\n",
      "112/112 [==============================] - 0s - loss: 0.1087 - acc: 0.9732     \n",
      "Epoch 173/200\n",
      "112/112 [==============================] - 0s - loss: 0.1071 - acc: 0.9732     \n",
      "Epoch 174/200\n",
      "112/112 [==============================] - 0s - loss: 0.1054 - acc: 0.9732     \n",
      "Epoch 175/200\n",
      "112/112 [==============================] - 0s - loss: 0.1035 - acc: 0.9732     \n",
      "Epoch 176/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s - loss: 0.1059 - acc: 0.9732     \n",
      "Epoch 177/200\n",
      "112/112 [==============================] - 0s - loss: 0.1049 - acc: 0.9732     \n",
      "Epoch 178/200\n",
      "112/112 [==============================] - 0s - loss: 0.1030 - acc: 0.9732     \n",
      "Epoch 179/200\n",
      "112/112 [==============================] - 0s - loss: 0.1044 - acc: 0.9732     \n",
      "Epoch 180/200\n",
      "112/112 [==============================] - 0s - loss: 0.1027 - acc: 0.9732     \n",
      "Epoch 181/200\n",
      "112/112 [==============================] - 0s - loss: 0.1027 - acc: 0.9732     \n",
      "Epoch 182/200\n",
      "112/112 [==============================] - 0s - loss: 0.1006 - acc: 0.9821     \n",
      "Epoch 183/200\n",
      "112/112 [==============================] - 0s - loss: 0.1008 - acc: 0.9821     \n",
      "Epoch 184/200\n",
      "112/112 [==============================] - 0s - loss: 0.1026 - acc: 0.9732     \n",
      "Epoch 185/200\n",
      "112/112 [==============================] - 0s - loss: 0.1004 - acc: 0.9732     \n",
      "Epoch 186/200\n",
      "112/112 [==============================] - 0s - loss: 0.1006 - acc: 0.9732     \n",
      "Epoch 187/200\n",
      "112/112 [==============================] - 0s - loss: 0.0996 - acc: 0.9732     \n",
      "Epoch 188/200\n",
      "112/112 [==============================] - 0s - loss: 0.0997 - acc: 0.9732     \n",
      "Epoch 189/200\n",
      "112/112 [==============================] - 0s - loss: 0.0990 - acc: 0.9732     \n",
      "Epoch 190/200\n",
      "112/112 [==============================] - 0s - loss: 0.0995 - acc: 0.9732     \n",
      "Epoch 191/200\n",
      "112/112 [==============================] - 0s - loss: 0.0999 - acc: 0.9732     \n",
      "Epoch 192/200\n",
      "112/112 [==============================] - 0s - loss: 0.0988 - acc: 0.9732     \n",
      "Epoch 193/200\n",
      "112/112 [==============================] - 0s - loss: 0.0943 - acc: 0.9732     \n",
      "Epoch 194/200\n",
      "112/112 [==============================] - 0s - loss: 0.0988 - acc: 0.9821     \n",
      "Epoch 195/200\n",
      "112/112 [==============================] - 0s - loss: 0.0963 - acc: 0.9732     \n",
      "Epoch 196/200\n",
      "112/112 [==============================] - 0s - loss: 0.0951 - acc: 0.9732     \n",
      "Epoch 197/200\n",
      "112/112 [==============================] - 0s - loss: 0.0951 - acc: 0.9821     \n",
      "Epoch 198/200\n",
      "112/112 [==============================] - 0s - loss: 0.0959 - acc: 0.9821     \n",
      "Epoch 199/200\n",
      "112/112 [==============================] - 0s - loss: 0.0951 - acc: 0.9732     \n",
      "Epoch 200/200\n",
      "112/112 [==============================] - 0s - loss: 0.0931 - acc: 0.9732     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f60ec255b70>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,epochs=200,batch_size=5,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test,y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.11879219408882291, 0.9736842105263158]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把model序列化为json对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.json','w') as f:\n",
    "    f.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"class_name\": \"Sequential\", \"config\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_4\", \"trainable\": true, \"batch_input_shape\": [null, 4], \"dtype\": \"float32\", \"units\": 4, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_5\", \"trainable\": true, \"units\": 6, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_6\", \"trainable\": true, \"units\": 3, \"activation\": \"softmax\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}], \"keras_version\": \"2.0.8\", \"backend\": \"tensorflow\"}\n"
     ]
    }
   ],
   "source": [
    "print(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存权值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model.json.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 已经保存了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从json中加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./model.json','r') as f:\n",
    "    model_json = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = model_from_json(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.load_weights('model.json.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_scores = new_model.evaluate(X_test,y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.11879219408882291, 0.9736842105263158]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 从json中加载出来的模型与之前的模型的结果是一致的。\n",
    "- 一个三层的感知机可以达到97.33%的准确率，已经很高了，还要啥自行车@@"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. yaml序列化模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与json的一致，只过不序列化的模型的保存的文件类型不同而已"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 模型增量更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 为了保证模型的时效性，需要时期对模型进行更新，通常是3~6个月，甚至是1~2个月。在数据量非常大时，若每次用全量数据重新训练，时间开销比较大，因此可以采用增量更新的方式。\n",
    "- 如果是基于时间序列的预测，增量更新相当于默认给新数据增加了权重，模型的准确度相对会比较好\n",
    "- 在实际的应用当中，如果采用增量更新模型，需要做与全量更新的对比实验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_old = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f60e4546cf8>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_old.fit(X_train,y_train,epochs=200,batch_size=5,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.06133596173354557, 0.9910714285714286]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_old.evaluate(X_train,y_train,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_model = model_old.to_yaml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.increment.yaml','w') as f:\n",
    "    f.write(yaml_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_old.save_weights('model.increment.yaml.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./model.increment.yaml','r') as f:\n",
    "    yaml_model_new = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = model_from_yaml(yaml_model_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new.load_weights('./model.increment.yaml.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "38/38 [==============================] - 0s - loss: 0.1105 - acc: 0.9737     \n",
      "Epoch 2/10\n",
      "38/38 [==============================] - 0s - loss: 0.1082 - acc: 0.9737     \n",
      "Epoch 3/10\n",
      "38/38 [==============================] - 0s - loss: 0.1115 - acc: 0.9737     \n",
      "Epoch 4/10\n",
      "38/38 [==============================] - 0s - loss: 0.1133 - acc: 0.9737     \n",
      "Epoch 5/10\n",
      "38/38 [==============================] - 0s - loss: 0.1188 - acc: 0.9474     \n",
      "Epoch 6/10\n",
      "38/38 [==============================] - 0s - loss: 0.1191 - acc: 0.9474     \n",
      "Epoch 7/10\n",
      "38/38 [==============================] - 0s - loss: 0.1108 - acc: 0.9474     \n",
      "Epoch 8/10\n",
      "38/38 [==============================] - 0s - loss: 0.1037 - acc: 0.9737     \n",
      "Epoch 9/10\n",
      "38/38 [==============================] - 0s - loss: 0.1041 - acc: 0.9737     \n",
      "Epoch 10/10\n",
      "38/38 [==============================] - 0s - loss: 0.1020 - acc: 0.9737     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f60e4546be0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_new.fit(X_test,y_test，verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.10203825927486546, 0.9736842105263158]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_new.evaluate(X_test,y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. checkpoint机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在Keras中，回调API提供checkpoint机制，ModelCheckpoint保存的就是模型的权重值，与save_weights()保存的文件是一致的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cp = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/200\n",
      "  5/120 [>.............................] - ETA: 4s - loss: 1.1101 - acc: 0.8000Epoch 00000: val_acc improved from -inf to 0.00000, saving model to weight-improvment-00-0.00.h5\n",
      "120/120 [==============================] - 0s - loss: 1.1175 - acc: 0.4000 - val_loss: 1.2416 - val_acc: 0.0000e+00\n",
      "Epoch 2/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 1.0957 - acc: 0.4000Epoch 00001: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 1.0826 - acc: 0.3833 - val_loss: 1.2632 - val_acc: 0.0000e+00\n",
      "Epoch 3/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 1.0508 - acc: 0.6000Epoch 00002: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 1.0580 - acc: 0.4167 - val_loss: 1.2830 - val_acc: 0.0000e+00\n",
      "Epoch 4/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 1.0907 - acc: 0.4000Epoch 00003: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 1.0362 - acc: 0.4167 - val_loss: 1.3128 - val_acc: 0.0000e+00\n",
      "Epoch 5/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 1.0310 - acc: 0.6000Epoch 00004: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 1.0194 - acc: 0.4167 - val_loss: 1.3351 - val_acc: 0.0000e+00\n",
      "Epoch 6/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 1.1301 - acc: 0.2000Epoch 00005: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 1.0036 - acc: 0.4167 - val_loss: 1.3476 - val_acc: 0.0000e+00\n",
      "Epoch 7/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.8548 - acc: 0.8000Epoch 00006: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.9886 - acc: 0.4333 - val_loss: 1.3586 - val_acc: 0.0000e+00\n",
      "Epoch 8/200\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.9802 - acc: 0.4348Epoch 00007: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.9735 - acc: 0.4417 - val_loss: 1.3659 - val_acc: 0.0000e+00\n",
      "Epoch 9/200\n",
      "100/120 [========================>.....] - ETA: 0s - loss: 0.9566 - acc: 0.4900Epoch 00008: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.9583 - acc: 0.4917 - val_loss: 1.3692 - val_acc: 0.0000e+00\n",
      "Epoch 10/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.8961 - acc: 0.6000Epoch 00009: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.9419 - acc: 0.5417 - val_loss: 1.3631 - val_acc: 0.0000e+00\n",
      "Epoch 11/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.7941 - acc: 0.8000Epoch 00010: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.9254 - acc: 0.6583 - val_loss: 1.3623 - val_acc: 0.0000e+00\n",
      "Epoch 12/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 1.1557 - acc: 0.4000Epoch 00011: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.9064 - acc: 0.7667 - val_loss: 1.3656 - val_acc: 0.0000e+00\n",
      "Epoch 13/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.8470 - acc: 0.8000Epoch 00012: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.8878 - acc: 0.8000 - val_loss: 1.3666 - val_acc: 0.0000e+00\n",
      "Epoch 14/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.7901 - acc: 1.0000Epoch 00013: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.8694 - acc: 0.8083 - val_loss: 1.3615 - val_acc: 0.0000e+00\n",
      "Epoch 15/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.6893 - acc: 1.0000Epoch 00014: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.8515 - acc: 0.8167 - val_loss: 1.3547 - val_acc: 0.0000e+00\n",
      "Epoch 16/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.9233 - acc: 0.6000Epoch 00015: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.8330 - acc: 0.8333 - val_loss: 1.3459 - val_acc: 0.0000e+00\n",
      "Epoch 17/200\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.8202 - acc: 0.8261Epoch 00016: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.8149 - acc: 0.8333 - val_loss: 1.3368 - val_acc: 0.0000e+00\n",
      "Epoch 18/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.7631 - acc: 0.8000Epoch 00017: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.7976 - acc: 0.8333 - val_loss: 1.3332 - val_acc: 0.0000e+00\n",
      "Epoch 19/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.9225 - acc: 0.6000Epoch 00018: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.7802 - acc: 0.8333 - val_loss: 1.3261 - val_acc: 0.0000e+00\n",
      "Epoch 20/200\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.7691 - acc: 0.8261Epoch 00019: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.7625 - acc: 0.8333 - val_loss: 1.3115 - val_acc: 0.0000e+00\n",
      "Epoch 21/200\n",
      "100/120 [========================>.....] - ETA: 0s - loss: 0.7466 - acc: 0.8400Epoch 00020: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.7447 - acc: 0.8333 - val_loss: 1.2930 - val_acc: 0.0000e+00\n",
      "Epoch 22/200\n",
      "105/120 [=========================>....] - ETA: 0s - loss: 0.7259 - acc: 0.8381Epoch 00021: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.7272 - acc: 0.8333 - val_loss: 1.2785 - val_acc: 0.0000e+00\n",
      "Epoch 23/200\n",
      " 80/120 [===================>..........] - ETA: 0s - loss: 0.7154 - acc: 0.8250Epoch 00022: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.7096 - acc: 0.8333 - val_loss: 1.2668 - val_acc: 0.0000e+00\n",
      "Epoch 24/200\n",
      " 95/120 [======================>.......] - ETA: 0s - loss: 0.7027 - acc: 0.8211Epoch 00023: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.6923 - acc: 0.8333 - val_loss: 1.2547 - val_acc: 0.0000e+00\n",
      "Epoch 25/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.7167 - acc: 0.8000Epoch 00024: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.6759 - acc: 0.8333 - val_loss: 1.2456 - val_acc: 0.0000e+00\n",
      "Epoch 26/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.6387 - acc: 0.8000Epoch 00025: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.6602 - acc: 0.8333 - val_loss: 1.2358 - val_acc: 0.0000e+00\n",
      "Epoch 27/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.5085 - acc: 1.0000Epoch 00026: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.6446 - acc: 0.8333 - val_loss: 1.2120 - val_acc: 0.0000e+00\n",
      "Epoch 28/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.6528 - acc: 0.8000Epoch 00027: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.6288 - acc: 0.8333 - val_loss: 1.2006 - val_acc: 0.0000e+00\n",
      "Epoch 29/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.6880 - acc: 0.8000Epoch 00028: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.6141 - acc: 0.8333 - val_loss: 1.1921 - val_acc: 0.0000e+00\n",
      "Epoch 30/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.6082 - acc: 0.8000Epoch 00029: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.5998 - acc: 0.8333 - val_loss: 1.1778 - val_acc: 0.0000e+00\n",
      "Epoch 31/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.7866 - acc: 0.6000Epoch 00030: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.5858 - acc: 0.8333 - val_loss: 1.1744 - val_acc: 0.0000e+00\n",
      "Epoch 32/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.4297 - acc: 1.0000Epoch 00031: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.5722 - acc: 0.8333 - val_loss: 1.1561 - val_acc: 0.0000e+00\n",
      "Epoch 33/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.5838 - acc: 0.8000Epoch 00032: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.5592 - acc: 0.8333 - val_loss: 1.1549 - val_acc: 0.0000e+00\n",
      "Epoch 34/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/120 [>.............................] - ETA: 0s - loss: 0.7191 - acc: 0.6000Epoch 00033: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.5470 - acc: 0.8333 - val_loss: 1.1532 - val_acc: 0.0000e+00\n",
      "Epoch 35/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.5762 - acc: 0.8000Epoch 00034: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.5349 - acc: 0.8333 - val_loss: 1.1276 - val_acc: 0.0000e+00\n",
      "Epoch 36/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.3980 - acc: 1.0000Epoch 00035: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.5239 - acc: 0.8333 - val_loss: 1.1296 - val_acc: 0.0000e+00\n",
      "Epoch 37/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.5484 - acc: 0.8000Epoch 00036: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.5120 - acc: 0.8333 - val_loss: 1.1236 - val_acc: 0.0000e+00\n",
      "Epoch 38/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.5638 - acc: 0.8000Epoch 00037: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.5015 - acc: 0.8333 - val_loss: 1.1286 - val_acc: 0.0000e+00\n",
      "Epoch 39/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.7094 - acc: 0.6000Epoch 00038: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.4925 - acc: 0.8333 - val_loss: 1.1150 - val_acc: 0.0000e+00\n",
      "Epoch 40/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.5485 - acc: 0.8000Epoch 00039: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.4826 - acc: 0.8333 - val_loss: 1.1052 - val_acc: 0.0000e+00\n",
      "Epoch 41/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.3015 - acc: 1.0000Epoch 00040: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.4739 - acc: 0.8333 - val_loss: 1.0980 - val_acc: 0.0000e+00\n",
      "Epoch 42/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.4069 - acc: 1.0000Epoch 00041: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.4654 - acc: 0.8333 - val_loss: 1.0821 - val_acc: 0.0000e+00\n",
      "Epoch 43/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.4306 - acc: 0.8000Epoch 00042: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.4572 - acc: 0.8333 - val_loss: 1.0848 - val_acc: 0.0000e+00\n",
      "Epoch 44/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2861 - acc: 1.0000Epoch 00043: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.4504 - acc: 0.8333 - val_loss: 1.0829 - val_acc: 0.0000e+00\n",
      "Epoch 45/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.4667 - acc: 0.8000Epoch 00044: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.4429 - acc: 0.8333 - val_loss: 1.0736 - val_acc: 0.0000e+00\n",
      "Epoch 46/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.4114 - acc: 0.8000Epoch 00045: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.4364 - acc: 0.8333 - val_loss: 1.0752 - val_acc: 0.0000e+00\n",
      "Epoch 47/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2034 - acc: 1.0000Epoch 00046: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.4308 - acc: 0.8333 - val_loss: 1.0729 - val_acc: 0.0000e+00\n",
      "Epoch 48/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.7652 - acc: 0.4000Epoch 00047: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.4246 - acc: 0.8333 - val_loss: 1.0737 - val_acc: 0.0000e+00\n",
      "Epoch 49/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.4496 - acc: 0.8000Epoch 00048: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.4187 - acc: 0.8333 - val_loss: 1.0760 - val_acc: 0.0000e+00\n",
      "Epoch 50/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.4545 - acc: 0.8000Epoch 00049: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.4135 - acc: 0.8333 - val_loss: 1.0743 - val_acc: 0.0000e+00\n",
      "Epoch 51/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2490 - acc: 1.0000Epoch 00050: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.4091 - acc: 0.8333 - val_loss: 1.0805 - val_acc: 0.0000e+00\n",
      "Epoch 52/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.3754 - acc: 0.8000Epoch 00051: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.4044 - acc: 0.8333 - val_loss: 1.0737 - val_acc: 0.0000e+00\n",
      "Epoch 53/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2266 - acc: 1.0000Epoch 00052: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.4002 - acc: 0.8333 - val_loss: 1.0616 - val_acc: 0.0000e+00\n",
      "Epoch 54/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.5027 - acc: 0.8000Epoch 00053: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3966 - acc: 0.8333 - val_loss: 1.0647 - val_acc: 0.0000e+00\n",
      "Epoch 55/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.4884 - acc: 0.8000Epoch 00054: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3920 - acc: 0.8333 - val_loss: 1.0498 - val_acc: 0.0000e+00\n",
      "Epoch 56/200\n",
      "110/120 [==========================>...] - ETA: 0s - loss: 0.3869 - acc: 0.8364Epoch 00055: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3892 - acc: 0.8333 - val_loss: 1.0592 - val_acc: 0.0000e+00\n",
      "Epoch 57/200\n",
      "100/120 [========================>.....] - ETA: 0s - loss: 0.4009 - acc: 0.8200Epoch 00056: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3861 - acc: 0.8333 - val_loss: 1.0601 - val_acc: 0.0000e+00\n",
      "Epoch 58/200\n",
      "100/120 [========================>.....] - ETA: 0s - loss: 0.3579 - acc: 0.8700Epoch 00057: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3829 - acc: 0.8333 - val_loss: 1.0491 - val_acc: 0.0000e+00\n",
      "Epoch 59/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.4876 - acc: 0.8000Epoch 00058: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3797 - acc: 0.8333 - val_loss: 1.0469 - val_acc: 0.0000e+00\n",
      "Epoch 60/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.4038 - acc: 0.8000Epoch 00059: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3778 - acc: 0.8333 - val_loss: 1.0448 - val_acc: 0.0000e+00\n",
      "Epoch 61/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2061 - acc: 1.0000Epoch 00060: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3760 - acc: 0.8333 - val_loss: 1.0604 - val_acc: 0.0000e+00\n",
      "Epoch 62/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.7398 - acc: 0.4000Epoch 00061: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3728 - acc: 0.8333 - val_loss: 1.0557 - val_acc: 0.0000e+00\n",
      "Epoch 63/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.3979 - acc: 0.8000Epoch 00062: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3705 - acc: 0.8333 - val_loss: 1.0489 - val_acc: 0.0000e+00\n",
      "Epoch 64/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.3139 - acc: 0.8000Epoch 00063: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3688 - acc: 0.8333 - val_loss: 1.0441 - val_acc: 0.0000e+00\n",
      "Epoch 65/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1064 - acc: 1.0000Epoch 00064: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3668 - acc: 0.8333 - val_loss: 1.0357 - val_acc: 0.0000e+00\n",
      "Epoch 66/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2704 - acc: 1.0000Epoch 00065: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3652 - acc: 0.8333 - val_loss: 1.0477 - val_acc: 0.0000e+00\n",
      "Epoch 67/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1872 - acc: 1.0000Epoch 00066: val_acc did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s - loss: 0.3639 - acc: 0.8333 - val_loss: 1.0353 - val_acc: 0.0000e+00\n",
      "Epoch 68/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.4712 - acc: 0.8000Epoch 00067: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3617 - acc: 0.8333 - val_loss: 1.0245 - val_acc: 0.0000e+00\n",
      "Epoch 69/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.3531 - acc: 1.0000Epoch 00068: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3609 - acc: 0.8333 - val_loss: 1.0171 - val_acc: 0.0000e+00\n",
      "Epoch 70/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1060 - acc: 1.0000Epoch 00069: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3592 - acc: 0.8333 - val_loss: 1.0119 - val_acc: 0.0000e+00\n",
      "Epoch 71/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2141 - acc: 0.8000Epoch 00070: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3580 - acc: 0.8333 - val_loss: 1.0152 - val_acc: 0.0000e+00\n",
      "Epoch 72/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2974 - acc: 0.8000Epoch 00071: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3568 - acc: 0.8333 - val_loss: 1.0138 - val_acc: 0.0000e+00\n",
      "Epoch 73/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.4871 - acc: 0.6000Epoch 00072: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3556 - acc: 0.8333 - val_loss: 1.0147 - val_acc: 0.0000e+00\n",
      "Epoch 74/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2618 - acc: 1.0000Epoch 00073: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3543 - acc: 0.8333 - val_loss: 1.0155 - val_acc: 0.0000e+00\n",
      "Epoch 75/200\n",
      "100/120 [========================>.....] - ETA: 0s - loss: 0.3359 - acc: 0.8400Epoch 00074: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3530 - acc: 0.8333 - val_loss: 1.0116 - val_acc: 0.0000e+00\n",
      "Epoch 76/200\n",
      " 80/120 [===================>..........] - ETA: 0s - loss: 0.3636 - acc: 0.8250Epoch 00075: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3517 - acc: 0.8333 - val_loss: 1.0105 - val_acc: 0.0000e+00\n",
      "Epoch 77/200\n",
      " 85/120 [====================>.........] - ETA: 0s - loss: 0.3348 - acc: 0.8471Epoch 00076: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3498 - acc: 0.8333 - val_loss: 1.0021 - val_acc: 0.0000e+00\n",
      "Epoch 78/200\n",
      " 90/120 [=====================>........] - ETA: 0s - loss: 0.3711 - acc: 0.8111Epoch 00077: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3474 - acc: 0.8333 - val_loss: 1.0030 - val_acc: 0.0000e+00\n",
      "Epoch 79/200\n",
      "110/120 [==========================>...] - ETA: 0s - loss: 0.3384 - acc: 0.8364Epoch 00078: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3434 - acc: 0.8333 - val_loss: 1.0062 - val_acc: 0.0000e+00\n",
      "Epoch 80/200\n",
      " 90/120 [=====================>........] - ETA: 0s - loss: 0.3385 - acc: 0.8333Epoch 00079: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3392 - acc: 0.8333 - val_loss: 0.9956 - val_acc: 0.0000e+00\n",
      "Epoch 81/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.4696 - acc: 0.6000Epoch 00080: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3351 - acc: 0.8333 - val_loss: 0.9935 - val_acc: 0.0000e+00\n",
      "Epoch 82/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.4322 - acc: 0.8000Epoch 00081: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3314 - acc: 0.8333 - val_loss: 0.9896 - val_acc: 0.0000e+00\n",
      "Epoch 83/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.3329 - acc: 1.0000Epoch 00082: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3266 - acc: 0.8333 - val_loss: 0.9522 - val_acc: 0.0000e+00\n",
      "Epoch 84/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.0894 - acc: 1.0000Epoch 00083: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3227 - acc: 0.8333 - val_loss: 0.9883 - val_acc: 0.0000e+00\n",
      "Epoch 85/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.3000 - acc: 1.0000Epoch 00084: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3213 - acc: 0.8333 - val_loss: 0.9747 - val_acc: 0.0000e+00\n",
      "Epoch 86/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.6692 - acc: 0.4000Epoch 00085: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3166 - acc: 0.8333 - val_loss: 0.9867 - val_acc: 0.0000e+00\n",
      "Epoch 87/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.4685 - acc: 0.8000Epoch 00086: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3146 - acc: 0.8333 - val_loss: 0.9654 - val_acc: 0.0000e+00\n",
      "Epoch 88/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.5566 - acc: 0.6000Epoch 00087: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3112 - acc: 0.8333 - val_loss: 0.9715 - val_acc: 0.0000e+00\n",
      "Epoch 89/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.4842 - acc: 0.6000Epoch 00088: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3078 - acc: 0.8333 - val_loss: 0.9449 - val_acc: 0.0000e+00\n",
      "Epoch 90/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.3020 - acc: 0.8000Epoch 00089: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3053 - acc: 0.8333 - val_loss: 0.9348 - val_acc: 0.0000e+00\n",
      "Epoch 91/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.5648 - acc: 0.6000Epoch 00090: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.3020 - acc: 0.8417 - val_loss: 0.9322 - val_acc: 0.0000e+00\n",
      "Epoch 92/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.3203 - acc: 0.8000Epoch 00091: val_acc improved from 0.00000 to 0.03333, saving model to weight-improvment-91-0.03.h5\n",
      "120/120 [==============================] - 0s - loss: 0.2985 - acc: 0.8500 - val_loss: 0.9175 - val_acc: 0.0333\n",
      "Epoch 93/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.3369 - acc: 1.0000Epoch 00092: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2960 - acc: 0.8500 - val_loss: 0.9194 - val_acc: 0.0333\n",
      "Epoch 94/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2062 - acc: 1.0000Epoch 00093: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2936 - acc: 0.8500 - val_loss: 0.9136 - val_acc: 0.0333\n",
      "Epoch 95/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.5369 - acc: 0.6000Epoch 00094: val_acc improved from 0.03333 to 0.10000, saving model to weight-improvment-94-0.10.h5\n",
      "120/120 [==============================] - 0s - loss: 0.2902 - acc: 0.8583 - val_loss: 0.9029 - val_acc: 0.1000\n",
      "Epoch 96/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2142 - acc: 1.0000Epoch 00095: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2885 - acc: 0.8500 - val_loss: 0.8967 - val_acc: 0.1000\n",
      "Epoch 97/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.3190 - acc: 0.8000Epoch 00096: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2856 - acc: 0.8667 - val_loss: 0.8868 - val_acc: 0.1000\n",
      "Epoch 98/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2874 - acc: 0.8000Epoch 00097: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2822 - acc: 0.8583 - val_loss: 0.8682 - val_acc: 0.1000\n",
      "Epoch 99/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1379 - acc: 1.0000Epoch 00098: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2791 - acc: 0.8667 - val_loss: 0.8522 - val_acc: 0.1000\n",
      "Epoch 100/200\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.2836 - acc: 0.8783Epoch 00099: val_acc did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s - loss: 0.2769 - acc: 0.8833 - val_loss: 0.8671 - val_acc: 0.1000\n",
      "Epoch 101/200\n",
      "100/120 [========================>.....] - ETA: 0s - loss: 0.2946 - acc: 0.8600Epoch 00100: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2741 - acc: 0.8833 - val_loss: 0.8806 - val_acc: 0.1000\n",
      "Epoch 102/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.4516 - acc: 0.8000Epoch 00101: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2717 - acc: 0.8750 - val_loss: 0.8718 - val_acc: 0.1000\n",
      "Epoch 103/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.5189 - acc: 0.6000Epoch 00102: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2680 - acc: 0.8750 - val_loss: 0.8760 - val_acc: 0.1000\n",
      "Epoch 104/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.3245 - acc: 0.8000Epoch 00103: val_acc improved from 0.10000 to 0.13333, saving model to weight-improvment-103-0.13.h5\n",
      "120/120 [==============================] - 0s - loss: 0.2663 - acc: 0.8833 - val_loss: 0.8543 - val_acc: 0.1333\n",
      "Epoch 105/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.3036 - acc: 0.8000Epoch 00104: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2632 - acc: 0.8833 - val_loss: 0.8521 - val_acc: 0.1333\n",
      "Epoch 106/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2328 - acc: 0.8000Epoch 00105: val_acc improved from 0.13333 to 0.16667, saving model to weight-improvment-105-0.17.h5\n",
      "120/120 [==============================] - 0s - loss: 0.2606 - acc: 0.9083 - val_loss: 0.8476 - val_acc: 0.1667\n",
      "Epoch 107/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2720 - acc: 0.8000Epoch 00106: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2582 - acc: 0.9000 - val_loss: 0.8437 - val_acc: 0.1667\n",
      "Epoch 108/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1219 - acc: 1.0000Epoch 00107: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2564 - acc: 0.8917 - val_loss: 0.8577 - val_acc: 0.1667\n",
      "Epoch 109/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.3479 - acc: 0.8000Epoch 00108: val_acc improved from 0.16667 to 0.30000, saving model to weight-improvment-108-0.30.h5\n",
      "120/120 [==============================] - 0s - loss: 0.2542 - acc: 0.8917 - val_loss: 0.8122 - val_acc: 0.3000\n",
      "Epoch 110/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2724 - acc: 1.0000Epoch 00109: val_acc improved from 0.30000 to 0.36667, saving model to weight-improvment-109-0.37.h5\n",
      "120/120 [==============================] - 0s - loss: 0.2509 - acc: 0.8917 - val_loss: 0.7718 - val_acc: 0.3667\n",
      "Epoch 111/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.3295 - acc: 0.8000Epoch 00110: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2489 - acc: 0.9083 - val_loss: 0.7845 - val_acc: 0.3333\n",
      "Epoch 112/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.0727 - acc: 1.0000Epoch 00111: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2462 - acc: 0.9083 - val_loss: 0.7749 - val_acc: 0.3667\n",
      "Epoch 113/200\n",
      "110/120 [==========================>...] - ETA: 0s - loss: 0.2382 - acc: 0.9091Epoch 00112: val_acc improved from 0.36667 to 0.40000, saving model to weight-improvment-112-0.40.h5\n",
      "120/120 [==============================] - 0s - loss: 0.2437 - acc: 0.9083 - val_loss: 0.7744 - val_acc: 0.4000\n",
      "Epoch 114/200\n",
      "110/120 [==========================>...] - ETA: 0s - loss: 0.2463 - acc: 0.9273Epoch 00113: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2420 - acc: 0.9250 - val_loss: 0.7811 - val_acc: 0.3667\n",
      "Epoch 115/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.3156 - acc: 1.0000Epoch 00114: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2393 - acc: 0.9083 - val_loss: 0.7644 - val_acc: 0.4000\n",
      "Epoch 116/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1642 - acc: 1.0000Epoch 00115: val_acc improved from 0.40000 to 0.43333, saving model to weight-improvment-115-0.43.h5\n",
      "120/120 [==============================] - 0s - loss: 0.2372 - acc: 0.9083 - val_loss: 0.7612 - val_acc: 0.4333\n",
      "Epoch 117/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2345 - acc: 1.0000Epoch 00116: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2350 - acc: 0.9250 - val_loss: 0.7756 - val_acc: 0.4000\n",
      "Epoch 118/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.3893 - acc: 0.8000Epoch 00117: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2323 - acc: 0.9417 - val_loss: 0.8053 - val_acc: 0.3333\n",
      "Epoch 119/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1461 - acc: 1.0000Epoch 00118: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2311 - acc: 0.9000 - val_loss: 0.7642 - val_acc: 0.4333\n",
      "Epoch 120/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2006 - acc: 1.0000Epoch 00119: val_acc improved from 0.43333 to 0.46667, saving model to weight-improvment-119-0.47.h5\n",
      "120/120 [==============================] - 0s - loss: 0.2273 - acc: 0.9167 - val_loss: 0.7063 - val_acc: 0.4667\n",
      "Epoch 121/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1315 - acc: 1.0000Epoch 00120: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2255 - acc: 0.9333 - val_loss: 0.7287 - val_acc: 0.4333\n",
      "Epoch 122/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1275 - acc: 1.0000Epoch 00121: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2233 - acc: 0.9333 - val_loss: 0.7341 - val_acc: 0.4333\n",
      "Epoch 123/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.4278 - acc: 0.8000Epoch 00122: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2213 - acc: 0.9333 - val_loss: 0.7453 - val_acc: 0.4333\n",
      "Epoch 124/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.3385 - acc: 0.8000Epoch 00123: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2194 - acc: 0.9167 - val_loss: 0.7348 - val_acc: 0.4333\n",
      "Epoch 125/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1886 - acc: 1.0000Epoch 00124: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2167 - acc: 0.9417 - val_loss: 0.7291 - val_acc: 0.4667\n",
      "Epoch 126/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2390 - acc: 1.0000Epoch 00125: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2139 - acc: 0.9417 - val_loss: 0.7060 - val_acc: 0.4667\n",
      "Epoch 127/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1223 - acc: 1.0000Epoch 00126: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2126 - acc: 0.9333 - val_loss: 0.6972 - val_acc: 0.4667\n",
      "Epoch 128/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1583 - acc: 1.0000Epoch 00127: val_acc improved from 0.46667 to 0.50000, saving model to weight-improvment-127-0.50.h5\n",
      "120/120 [==============================] - 0s - loss: 0.2106 - acc: 0.9333 - val_loss: 0.6734 - val_acc: 0.5000\n",
      "Epoch 129/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2249 - acc: 1.0000Epoch 00128: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2082 - acc: 0.9417 - val_loss: 0.6744 - val_acc: 0.5000\n",
      "Epoch 130/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2976 - acc: 0.8000Epoch 00129: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2062 - acc: 0.9500 - val_loss: 0.6878 - val_acc: 0.4667\n",
      "Epoch 131/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2486 - acc: 0.8000Epoch 00130: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2027 - acc: 0.9583 - val_loss: 0.7358 - val_acc: 0.4333\n",
      "Epoch 132/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2534 - acc: 1.0000Epoch 00131: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2029 - acc: 0.9417 - val_loss: 0.7062 - val_acc: 0.4667\n",
      "Epoch 133/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2290 - acc: 1.0000Epoch 00132: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.2008 - acc: 0.9417 - val_loss: 0.6770 - val_acc: 0.5000\n",
      "Epoch 134/200\n",
      "100/120 [========================>.....] - ETA: 0s - loss: 0.2002 - acc: 0.9600Epoch 00133: val_acc improved from 0.50000 to 0.53333, saving model to weight-improvment-133-0.53.h5\n",
      "120/120 [==============================] - 0s - loss: 0.1979 - acc: 0.9500 - val_loss: 0.6657 - val_acc: 0.5333\n",
      "Epoch 135/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2690 - acc: 1.0000Epoch 00134: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1979 - acc: 0.9417 - val_loss: 0.6579 - val_acc: 0.5333\n",
      "Epoch 136/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1219 - acc: 1.0000Epoch 00135: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1950 - acc: 0.9667 - val_loss: 0.6678 - val_acc: 0.5333\n",
      "Epoch 137/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1635 - acc: 1.0000Epoch 00136: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1934 - acc: 0.9500 - val_loss: 0.6803 - val_acc: 0.5000\n",
      "Epoch 138/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1794 - acc: 1.0000Epoch 00137: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1907 - acc: 0.9500 - val_loss: 0.6692 - val_acc: 0.5333\n",
      "Epoch 139/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2681 - acc: 1.0000Epoch 00138: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1892 - acc: 0.9667 - val_loss: 0.6722 - val_acc: 0.5333\n",
      "Epoch 140/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2942 - acc: 0.8000Epoch 00139: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1872 - acc: 0.9583 - val_loss: 0.6913 - val_acc: 0.4667\n",
      "Epoch 141/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.0333 - acc: 1.0000Epoch 00140: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1867 - acc: 0.9500 - val_loss: 0.6678 - val_acc: 0.5333\n",
      "Epoch 142/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1856 - acc: 1.0000Epoch 00141: val_acc improved from 0.53333 to 0.63333, saving model to weight-improvment-141-0.63.h5\n",
      "120/120 [==============================] - 0s - loss: 0.1839 - acc: 0.9583 - val_loss: 0.6223 - val_acc: 0.6333\n",
      "Epoch 143/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.0333 - acc: 1.0000Epoch 00142: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1827 - acc: 0.9667 - val_loss: 0.6370 - val_acc: 0.6333\n",
      "Epoch 144/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1862 - acc: 1.0000Epoch 00143: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1796 - acc: 0.9583 - val_loss: 0.6501 - val_acc: 0.5667\n",
      "Epoch 145/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1741 - acc: 1.0000Epoch 00144: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1785 - acc: 0.9500 - val_loss: 0.6398 - val_acc: 0.6333\n",
      "Epoch 146/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1923 - acc: 1.0000Epoch 00145: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1759 - acc: 0.9583 - val_loss: 0.6343 - val_acc: 0.6333\n",
      "Epoch 147/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.0758 - acc: 1.0000Epoch 00146: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1746 - acc: 0.9667 - val_loss: 0.6019 - val_acc: 0.6333\n",
      "Epoch 148/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1851 - acc: 1.0000Epoch 00147: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1735 - acc: 0.9583 - val_loss: 0.5970 - val_acc: 0.6333\n",
      "Epoch 149/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.3017 - acc: 0.8000Epoch 00148: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1717 - acc: 0.9750 - val_loss: 0.6183 - val_acc: 0.6333\n",
      "Epoch 150/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1966 - acc: 1.0000Epoch 00149: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1697 - acc: 0.9667 - val_loss: 0.6155 - val_acc: 0.6333\n",
      "Epoch 151/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1065 - acc: 1.0000Epoch 00150: val_acc improved from 0.63333 to 0.66667, saving model to weight-improvment-150-0.67.h5\n",
      "120/120 [==============================] - 0s - loss: 0.1685 - acc: 0.9500 - val_loss: 0.5863 - val_acc: 0.6667\n",
      "Epoch 152/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.0846 - acc: 1.0000Epoch 00151: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1665 - acc: 0.9750 - val_loss: 0.6106 - val_acc: 0.6333\n",
      "Epoch 153/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.0532 - acc: 1.0000Epoch 00152: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1638 - acc: 0.9667 - val_loss: 0.6454 - val_acc: 0.5667\n",
      "Epoch 154/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2193 - acc: 0.8000Epoch 00153: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1631 - acc: 0.9667 - val_loss: 0.6249 - val_acc: 0.6333\n",
      "Epoch 155/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1480 - acc: 1.0000Epoch 00154: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1644 - acc: 0.9500 - val_loss: 0.5984 - val_acc: 0.6333\n",
      "Epoch 156/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1789 - acc: 1.0000Epoch 00155: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1604 - acc: 0.9667 - val_loss: 0.5720 - val_acc: 0.6667\n",
      "Epoch 157/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2042 - acc: 1.0000Epoch 00156: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1586 - acc: 0.9583 - val_loss: 0.5780 - val_acc: 0.6667\n",
      "Epoch 158/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.0888 - acc: 1.0000Epoch 00157: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1567 - acc: 0.9750 - val_loss: 0.5864 - val_acc: 0.6667\n",
      "Epoch 159/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1388 - acc: 1.0000Epoch 00158: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1560 - acc: 0.9667 - val_loss: 0.5574 - val_acc: 0.6667\n",
      "Epoch 160/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1468 - acc: 1.0000Epoch 00159: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1540 - acc: 0.9833 - val_loss: 0.5801 - val_acc: 0.6667\n",
      "Epoch 161/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1640 - acc: 1.0000Epoch 00160: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1532 - acc: 0.9667 - val_loss: 0.5547 - val_acc: 0.6667\n",
      "Epoch 162/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1269 - acc: 1.0000Epoch 00161: val_acc improved from 0.66667 to 0.70000, saving model to weight-improvment-161-0.70.h5\n",
      "120/120 [==============================] - 0s - loss: 0.1511 - acc: 0.9750 - val_loss: 0.5433 - val_acc: 0.7000\n",
      "Epoch 163/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1769 - acc: 1.0000Epoch 00162: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1494 - acc: 0.9750 - val_loss: 0.5916 - val_acc: 0.6667\n",
      "Epoch 164/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.0724 - acc: 1.0000Epoch 00163: val_acc did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s - loss: 0.1495 - acc: 0.9667 - val_loss: 0.5583 - val_acc: 0.6667\n",
      "Epoch 165/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1025 - acc: 1.0000Epoch 00164: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1469 - acc: 0.9667 - val_loss: 0.5784 - val_acc: 0.6667\n",
      "Epoch 166/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.0285 - acc: 1.0000Epoch 00165: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1466 - acc: 0.9833 - val_loss: 0.5639 - val_acc: 0.6667\n",
      "Epoch 167/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2646 - acc: 1.0000Epoch 00166: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1447 - acc: 0.9667 - val_loss: 0.5631 - val_acc: 0.6667\n",
      "Epoch 168/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.3209 - acc: 1.0000Epoch 00167: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1436 - acc: 0.9750 - val_loss: 0.5624 - val_acc: 0.6667\n",
      "Epoch 169/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.0151 - acc: 1.0000Epoch 00168: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1424 - acc: 0.9750 - val_loss: 0.5192 - val_acc: 0.7000\n",
      "Epoch 170/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.0597 - acc: 1.0000Epoch 00169: val_acc improved from 0.70000 to 0.76667, saving model to weight-improvment-169-0.77.h5\n",
      "120/120 [==============================] - 0s - loss: 0.1400 - acc: 0.9750 - val_loss: 0.4838 - val_acc: 0.7667\n",
      "Epoch 171/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2325 - acc: 0.8000Epoch 00170: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1383 - acc: 0.9750 - val_loss: 0.5454 - val_acc: 0.6667\n",
      "Epoch 172/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2814 - acc: 1.0000Epoch 00171: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1374 - acc: 0.9750 - val_loss: 0.5587 - val_acc: 0.6667\n",
      "Epoch 173/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2706 - acc: 1.0000Epoch 00172: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1371 - acc: 0.9833 - val_loss: 0.5237 - val_acc: 0.7000\n",
      "Epoch 174/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1519 - acc: 1.0000Epoch 00173: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1366 - acc: 0.9667 - val_loss: 0.5151 - val_acc: 0.7000\n",
      "Epoch 175/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.0044 - acc: 1.0000Epoch 00174: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1348 - acc: 0.9667 - val_loss: 0.5238 - val_acc: 0.7000\n",
      "Epoch 176/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2673 - acc: 0.8000Epoch 00175: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1335 - acc: 0.9750 - val_loss: 0.5007 - val_acc: 0.7333\n",
      "Epoch 177/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1174 - acc: 1.0000Epoch 00176: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1326 - acc: 0.9667 - val_loss: 0.4995 - val_acc: 0.7333\n",
      "Epoch 178/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.0043 - acc: 1.0000Epoch 00177: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1313 - acc: 0.9667 - val_loss: 0.4985 - val_acc: 0.7333\n",
      "Epoch 179/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1103 - acc: 1.0000Epoch 00178: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1306 - acc: 0.9750 - val_loss: 0.4944 - val_acc: 0.7333\n",
      "Epoch 180/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1274 - acc: 1.0000Epoch 00179: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1288 - acc: 0.9750 - val_loss: 0.4871 - val_acc: 0.7333\n",
      "Epoch 181/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1456 - acc: 1.0000Epoch 00180: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1279 - acc: 0.9667 - val_loss: 0.4877 - val_acc: 0.7333\n",
      "Epoch 182/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.0622 - acc: 1.0000Epoch 00181: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1271 - acc: 0.9750 - val_loss: 0.4684 - val_acc: 0.7667\n",
      "Epoch 183/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2216 - acc: 1.0000Epoch 00182: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1258 - acc: 0.9667 - val_loss: 0.4594 - val_acc: 0.7667\n",
      "Epoch 184/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.0692 - acc: 1.0000Epoch 00183: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1242 - acc: 0.9750 - val_loss: 0.4886 - val_acc: 0.7333\n",
      "Epoch 185/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1164 - acc: 1.0000Epoch 00184: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1228 - acc: 0.9750 - val_loss: 0.4412 - val_acc: 0.7667\n",
      "Epoch 186/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.0817 - acc: 1.0000Epoch 00185: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1225 - acc: 0.9667 - val_loss: 0.4389 - val_acc: 0.7667\n",
      "Epoch 187/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.0815 - acc: 1.0000Epoch 00186: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1224 - acc: 0.9667 - val_loss: 0.4666 - val_acc: 0.7667\n",
      "Epoch 188/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.2753 - acc: 0.8000Epoch 00187: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1209 - acc: 0.9667 - val_loss: 0.4760 - val_acc: 0.7667\n",
      "Epoch 189/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1432 - acc: 1.0000Epoch 00188: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1205 - acc: 0.9667 - val_loss: 0.4703 - val_acc: 0.7667\n",
      "Epoch 190/200\n",
      "100/120 [========================>.....] - ETA: 0s - loss: 0.1266 - acc: 0.9700Epoch 00189: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1184 - acc: 0.9750 - val_loss: 0.4787 - val_acc: 0.7667\n",
      "Epoch 191/200\n",
      " 85/120 [====================>.........] - ETA: 0s - loss: 0.1087 - acc: 0.9765Epoch 00190: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1186 - acc: 0.9667 - val_loss: 0.4551 - val_acc: 0.7667\n",
      "Epoch 192/200\n",
      "105/120 [=========================>....] - ETA: 0s - loss: 0.1184 - acc: 0.9619Epoch 00191: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1171 - acc: 0.9667 - val_loss: 0.4570 - val_acc: 0.7667\n",
      "Epoch 193/200\n",
      "100/120 [========================>.....] - ETA: 0s - loss: 0.1235 - acc: 0.9600Epoch 00192: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1162 - acc: 0.9667 - val_loss: 0.4614 - val_acc: 0.7667\n",
      "Epoch 194/200\n",
      "100/120 [========================>.....] - ETA: 0s - loss: 0.1024 - acc: 0.9700Epoch 00193: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1155 - acc: 0.9667 - val_loss: 0.4352 - val_acc: 0.7667\n",
      "Epoch 195/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.0534 - acc: 1.0000Epoch 00194: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1153 - acc: 0.9750 - val_loss: 0.4373 - val_acc: 0.7667\n",
      "Epoch 196/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1167 - acc: 1.0000Epoch 00195: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1140 - acc: 0.9667 - val_loss: 0.4637 - val_acc: 0.7667\n",
      "Epoch 197/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.1413 - acc: 1.0000Epoch 00196: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1134 - acc: 0.9667 - val_loss: 0.4608 - val_acc: 0.7667\n",
      "Epoch 198/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/120 [>.............................] - ETA: 0s - loss: 0.0170 - acc: 1.0000Epoch 00197: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1130 - acc: 0.9667 - val_loss: 0.4729 - val_acc: 0.7667\n",
      "Epoch 199/200\n",
      "  5/120 [>.............................] - ETA: 0s - loss: 0.0459 - acc: 1.0000Epoch 00198: val_acc improved from 0.76667 to 0.80000, saving model to weight-improvment-198-0.80.h5\n",
      "120/120 [==============================] - 0s - loss: 0.1097 - acc: 0.9833 - val_loss: 0.3993 - val_acc: 0.8000\n",
      "Epoch 200/200\n",
      "105/120 [=========================>....] - ETA: 0s - loss: 0.1077 - acc: 0.9619Epoch 00199: val_acc did not improve\n",
      "120/120 [==============================] - 0s - loss: 0.1115 - acc: 0.9667 - val_loss: 0.4331 - val_acc: 0.7667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f60c85b6c18>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#设置checkpoint\n",
    "filepath = 'weight-improvment-{epoch:02d}-{val_acc:.2f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,monitor='val_acc',verbose=1,save_best_only=True,mode='max')\n",
    "callback_list = [checkpoint]\n",
    "model_cp.fit(X,y,validation_split=0.2,epochs=200,batch_size=5,verbose=1,callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用ModelCheckpoint在训练过程中，通过检查点保存了模型的权重值，当模型意外中止时，可以从checkpoint中加载和使用检查点的权重值。\n",
    "- 可以在模型训练前序列化模型的拓扑结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载checkpoint的模型权重值，与model.load_weights()一致，就不写代码了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 模型训练可视化\n",
    "- model.fit()返回的是一个History对象，里面包含了训练的历史记录，包括训练结查的loss和metrics指标，以及验证结果的loss和metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/200\n",
      "120/120 [==============================] - 0s - loss: 5.2314 - acc: 0.1667 - val_loss: 0.0128 - val_acc: 1.0000\n",
      "Epoch 2/200\n",
      "120/120 [==============================] - 0s - loss: 4.0934 - acc: 0.1667 - val_loss: 0.0416 - val_acc: 1.0000\n",
      "Epoch 3/200\n",
      "120/120 [==============================] - 0s - loss: 3.2298 - acc: 0.1667 - val_loss: 0.1091 - val_acc: 1.0000\n",
      "Epoch 4/200\n",
      "120/120 [==============================] - 0s - loss: 2.5334 - acc: 0.1667 - val_loss: 0.2359 - val_acc: 1.0000\n",
      "Epoch 5/200\n",
      "120/120 [==============================] - 0s - loss: 2.0187 - acc: 0.1667 - val_loss: 0.4202 - val_acc: 1.0000\n",
      "Epoch 6/200\n",
      "120/120 [==============================] - 0s - loss: 1.6444 - acc: 0.1667 - val_loss: 0.6312 - val_acc: 1.0000\n",
      "Epoch 7/200\n",
      "120/120 [==============================] - 0s - loss: 1.3979 - acc: 0.1417 - val_loss: 0.8244 - val_acc: 0.8667\n",
      "Epoch 8/200\n",
      "120/120 [==============================] - 0s - loss: 1.2364 - acc: 0.0500 - val_loss: 1.0269 - val_acc: 0.0333\n",
      "Epoch 9/200\n",
      "120/120 [==============================] - 0s - loss: 1.1285 - acc: 0.1333 - val_loss: 1.2194 - val_acc: 0.0000e+00\n",
      "Epoch 10/200\n",
      "120/120 [==============================] - 0s - loss: 1.0719 - acc: 0.4167 - val_loss: 1.3609 - val_acc: 0.0000e+00\n",
      "Epoch 11/200\n",
      "120/120 [==============================] - 0s - loss: 1.0432 - acc: 0.4167 - val_loss: 1.4472 - val_acc: 0.0000e+00\n",
      "Epoch 12/200\n",
      "120/120 [==============================] - 0s - loss: 1.0240 - acc: 0.4167 - val_loss: 1.5194 - val_acc: 0.0000e+00\n",
      "Epoch 13/200\n",
      "120/120 [==============================] - 0s - loss: 1.0072 - acc: 0.4500 - val_loss: 1.5688 - val_acc: 0.0000e+00\n",
      "Epoch 14/200\n",
      "120/120 [==============================] - 0s - loss: 0.9883 - acc: 0.5833 - val_loss: 1.5872 - val_acc: 0.0000e+00\n",
      "Epoch 15/200\n",
      "120/120 [==============================] - 0s - loss: 0.9680 - acc: 0.7083 - val_loss: 1.6125 - val_acc: 0.0000e+00\n",
      "Epoch 16/200\n",
      "120/120 [==============================] - 0s - loss: 0.9504 - acc: 0.7750 - val_loss: 1.6207 - val_acc: 0.0000e+00\n",
      "Epoch 17/200\n",
      "120/120 [==============================] - 0s - loss: 0.9314 - acc: 0.7500 - val_loss: 1.6392 - val_acc: 0.0000e+00\n",
      "Epoch 18/200\n",
      "120/120 [==============================] - 0s - loss: 0.9093 - acc: 0.7833 - val_loss: 1.6405 - val_acc: 0.0000e+00\n",
      "Epoch 19/200\n",
      "120/120 [==============================] - 0s - loss: 0.8826 - acc: 0.7583 - val_loss: 1.6581 - val_acc: 0.0000e+00\n",
      "Epoch 20/200\n",
      "120/120 [==============================] - 0s - loss: 0.8539 - acc: 0.7583 - val_loss: 1.6605 - val_acc: 0.0000e+00\n",
      "Epoch 21/200\n",
      "120/120 [==============================] - 0s - loss: 0.8251 - acc: 0.7750 - val_loss: 1.6651 - val_acc: 0.0000e+00\n",
      "Epoch 22/200\n",
      "120/120 [==============================] - 0s - loss: 0.7946 - acc: 0.7667 - val_loss: 1.6774 - val_acc: 0.0000e+00\n",
      "Epoch 23/200\n",
      "120/120 [==============================] - 0s - loss: 0.7645 - acc: 0.7917 - val_loss: 1.6832 - val_acc: 0.0000e+00\n",
      "Epoch 24/200\n",
      "120/120 [==============================] - 0s - loss: 0.7330 - acc: 0.8250 - val_loss: 1.6717 - val_acc: 0.0000e+00\n",
      "Epoch 25/200\n",
      "120/120 [==============================] - 0s - loss: 0.7047 - acc: 0.8167 - val_loss: 1.6768 - val_acc: 0.0000e+00\n",
      "Epoch 26/200\n",
      "120/120 [==============================] - 0s - loss: 0.6772 - acc: 0.8250 - val_loss: 1.6941 - val_acc: 0.0000e+00\n",
      "Epoch 27/200\n",
      "120/120 [==============================] - 0s - loss: 0.6510 - acc: 0.8250 - val_loss: 1.6731 - val_acc: 0.0000e+00\n",
      "Epoch 28/200\n",
      "120/120 [==============================] - 0s - loss: 0.6276 - acc: 0.8250 - val_loss: 1.6699 - val_acc: 0.0000e+00\n",
      "Epoch 29/200\n",
      "120/120 [==============================] - 0s - loss: 0.6040 - acc: 0.8333 - val_loss: 1.6913 - val_acc: 0.0000e+00\n",
      "Epoch 30/200\n",
      "120/120 [==============================] - 0s - loss: 0.5822 - acc: 0.8333 - val_loss: 1.6816 - val_acc: 0.0000e+00\n",
      "Epoch 31/200\n",
      "120/120 [==============================] - 0s - loss: 0.5596 - acc: 0.8333 - val_loss: 1.6881 - val_acc: 0.0000e+00\n",
      "Epoch 32/200\n",
      "120/120 [==============================] - 0s - loss: 0.5408 - acc: 0.8333 - val_loss: 1.6738 - val_acc: 0.0000e+00\n",
      "Epoch 33/200\n",
      "120/120 [==============================] - 0s - loss: 0.5228 - acc: 0.8333 - val_loss: 1.6863 - val_acc: 0.0000e+00\n",
      "Epoch 34/200\n",
      "120/120 [==============================] - 0s - loss: 0.5075 - acc: 0.8333 - val_loss: 1.6873 - val_acc: 0.0000e+00\n",
      "Epoch 35/200\n",
      "120/120 [==============================] - 0s - loss: 0.4919 - acc: 0.8333 - val_loss: 1.6909 - val_acc: 0.0000e+00\n",
      "Epoch 36/200\n",
      "120/120 [==============================] - 0s - loss: 0.4795 - acc: 0.8333 - val_loss: 1.6733 - val_acc: 0.0000e+00\n",
      "Epoch 37/200\n",
      "120/120 [==============================] - 0s - loss: 0.4662 - acc: 0.8333 - val_loss: 1.6610 - val_acc: 0.0000e+00\n",
      "Epoch 38/200\n",
      "120/120 [==============================] - 0s - loss: 0.4538 - acc: 0.8333 - val_loss: 1.6460 - val_acc: 0.0000e+00\n",
      "Epoch 39/200\n",
      "120/120 [==============================] - 0s - loss: 0.4422 - acc: 0.8333 - val_loss: 1.6302 - val_acc: 0.0000e+00\n",
      "Epoch 40/200\n",
      "120/120 [==============================] - 0s - loss: 0.4319 - acc: 0.8333 - val_loss: 1.6284 - val_acc: 0.0000e+00\n",
      "Epoch 41/200\n",
      "120/120 [==============================] - 0s - loss: 0.4232 - acc: 0.8333 - val_loss: 1.6462 - val_acc: 0.0000e+00\n",
      "Epoch 42/200\n",
      "120/120 [==============================] - 0s - loss: 0.4128 - acc: 0.8333 - val_loss: 1.6377 - val_acc: 0.0000e+00\n",
      "Epoch 43/200\n",
      "120/120 [==============================] - 0s - loss: 0.4045 - acc: 0.8333 - val_loss: 1.6249 - val_acc: 0.0000e+00\n",
      "Epoch 44/200\n",
      "120/120 [==============================] - 0s - loss: 0.3967 - acc: 0.8333 - val_loss: 1.6058 - val_acc: 0.0000e+00\n",
      "Epoch 45/200\n",
      "120/120 [==============================] - 0s - loss: 0.3882 - acc: 0.8333 - val_loss: 1.5665 - val_acc: 0.0000e+00\n",
      "Epoch 46/200\n",
      "120/120 [==============================] - 0s - loss: 0.3797 - acc: 0.8333 - val_loss: 1.5258 - val_acc: 0.0000e+00\n",
      "Epoch 47/200\n",
      "120/120 [==============================] - 0s - loss: 0.3738 - acc: 0.8333 - val_loss: 1.5477 - val_acc: 0.0000e+00\n",
      "Epoch 48/200\n",
      "120/120 [==============================] - 0s - loss: 0.3682 - acc: 0.8333 - val_loss: 1.5452 - val_acc: 0.0000e+00\n",
      "Epoch 49/200\n",
      "120/120 [==============================] - 0s - loss: 0.3625 - acc: 0.8333 - val_loss: 1.5389 - val_acc: 0.0000e+00\n",
      "Epoch 50/200\n",
      "120/120 [==============================] - 0s - loss: 0.3553 - acc: 0.8333 - val_loss: 1.5165 - val_acc: 0.0000e+00\n",
      "Epoch 51/200\n",
      "120/120 [==============================] - 0s - loss: 0.3496 - acc: 0.8333 - val_loss: 1.4893 - val_acc: 0.0000e+00\n",
      "Epoch 52/200\n",
      "120/120 [==============================] - 0s - loss: 0.3445 - acc: 0.8333 - val_loss: 1.4823 - val_acc: 0.0000e+00\n",
      "Epoch 53/200\n",
      "120/120 [==============================] - 0s - loss: 0.3385 - acc: 0.8333 - val_loss: 1.4730 - val_acc: 0.0000e+00\n",
      "Epoch 54/200\n",
      "120/120 [==============================] - 0s - loss: 0.3327 - acc: 0.8333 - val_loss: 1.4757 - val_acc: 0.0000e+00\n",
      "Epoch 55/200\n",
      "120/120 [==============================] - 0s - loss: 0.3270 - acc: 0.8333 - val_loss: 1.4254 - val_acc: 0.0000e+00\n",
      "Epoch 56/200\n",
      "120/120 [==============================] - 0s - loss: 0.3221 - acc: 0.8333 - val_loss: 1.4330 - val_acc: 0.0000e+00\n",
      "Epoch 57/200\n",
      "120/120 [==============================] - 0s - loss: 0.3168 - acc: 0.8333 - val_loss: 1.4211 - val_acc: 0.0000e+00\n",
      "Epoch 58/200\n",
      "120/120 [==============================] - 0s - loss: 0.3120 - acc: 0.8333 - val_loss: 1.4183 - val_acc: 0.0000e+00\n",
      "Epoch 59/200\n",
      "120/120 [==============================] - 0s - loss: 0.3065 - acc: 0.8333 - val_loss: 1.4007 - val_acc: 0.0000e+00\n",
      "Epoch 60/200\n",
      "120/120 [==============================] - 0s - loss: 0.3025 - acc: 0.8333 - val_loss: 1.3686 - val_acc: 0.0000e+00\n",
      "Epoch 61/200\n",
      "120/120 [==============================] - 0s - loss: 0.2952 - acc: 0.8333 - val_loss: 1.4021 - val_acc: 0.0000e+00\n",
      "Epoch 62/200\n",
      "120/120 [==============================] - 0s - loss: 0.2915 - acc: 0.8333 - val_loss: 1.3698 - val_acc: 0.0000e+00\n",
      "Epoch 63/200\n",
      "120/120 [==============================] - 0s - loss: 0.2870 - acc: 0.8333 - val_loss: 1.2904 - val_acc: 0.0000e+00\n",
      "Epoch 64/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s - loss: 0.2828 - acc: 0.8333 - val_loss: 1.2765 - val_acc: 0.0000e+00\n",
      "Epoch 65/200\n",
      "120/120 [==============================] - 0s - loss: 0.2799 - acc: 0.8333 - val_loss: 1.2733 - val_acc: 0.0000e+00\n",
      "Epoch 66/200\n",
      "120/120 [==============================] - 0s - loss: 0.2729 - acc: 0.8333 - val_loss: 1.2087 - val_acc: 0.0000e+00\n",
      "Epoch 67/200\n",
      "120/120 [==============================] - 0s - loss: 0.2689 - acc: 0.8333 - val_loss: 1.2108 - val_acc: 0.0000e+00\n",
      "Epoch 68/200\n",
      "120/120 [==============================] - 0s - loss: 0.2653 - acc: 0.8333 - val_loss: 1.2539 - val_acc: 0.0000e+00\n",
      "Epoch 69/200\n",
      "120/120 [==============================] - 0s - loss: 0.2620 - acc: 0.8333 - val_loss: 1.2227 - val_acc: 0.0000e+00\n",
      "Epoch 70/200\n",
      "120/120 [==============================] - 0s - loss: 0.2567 - acc: 0.8333 - val_loss: 1.2148 - val_acc: 0.0000e+00\n",
      "Epoch 71/200\n",
      "120/120 [==============================] - 0s - loss: 0.2528 - acc: 0.8333 - val_loss: 1.1392 - val_acc: 0.0000e+00\n",
      "Epoch 72/200\n",
      "120/120 [==============================] - 0s - loss: 0.2475 - acc: 0.8333 - val_loss: 1.1731 - val_acc: 0.0000e+00\n",
      "Epoch 73/200\n",
      "120/120 [==============================] - 0s - loss: 0.2435 - acc: 0.8333 - val_loss: 1.1217 - val_acc: 0.0000e+00\n",
      "Epoch 74/200\n",
      "120/120 [==============================] - 0s - loss: 0.2395 - acc: 0.8333 - val_loss: 1.0770 - val_acc: 0.0000e+00\n",
      "Epoch 75/200\n",
      "120/120 [==============================] - 0s - loss: 0.2376 - acc: 0.8333 - val_loss: 1.0710 - val_acc: 0.0000e+00\n",
      "Epoch 76/200\n",
      "120/120 [==============================] - 0s - loss: 0.2335 - acc: 0.8333 - val_loss: 1.0571 - val_acc: 0.0000e+00\n",
      "Epoch 77/200\n",
      "120/120 [==============================] - 0s - loss: 0.2285 - acc: 0.8333 - val_loss: 1.0718 - val_acc: 0.0000e+00\n",
      "Epoch 78/200\n",
      "120/120 [==============================] - 0s - loss: 0.2253 - acc: 0.8500 - val_loss: 1.0120 - val_acc: 0.5000\n",
      "Epoch 79/200\n",
      "120/120 [==============================] - 0s - loss: 0.2186 - acc: 0.9250 - val_loss: 1.1422 - val_acc: 0.4333\n",
      "Epoch 80/200\n",
      "120/120 [==============================] - 0s - loss: 0.2170 - acc: 0.9250 - val_loss: 1.1354 - val_acc: 0.4333\n",
      "Epoch 81/200\n",
      "120/120 [==============================] - 0s - loss: 0.2142 - acc: 0.9083 - val_loss: 1.0030 - val_acc: 0.5333\n",
      "Epoch 82/200\n",
      "120/120 [==============================] - 0s - loss: 0.2102 - acc: 0.9417 - val_loss: 1.0644 - val_acc: 0.4667\n",
      "Epoch 83/200\n",
      "120/120 [==============================] - 0s - loss: 0.2066 - acc: 0.9167 - val_loss: 1.0411 - val_acc: 0.5000\n",
      "Epoch 84/200\n",
      "120/120 [==============================] - 0s - loss: 0.2034 - acc: 0.9250 - val_loss: 0.9640 - val_acc: 0.6000\n",
      "Epoch 85/200\n",
      "120/120 [==============================] - 0s - loss: 0.1987 - acc: 0.9417 - val_loss: 0.9077 - val_acc: 0.6667\n",
      "Epoch 86/200\n",
      "120/120 [==============================] - 0s - loss: 0.1972 - acc: 0.9417 - val_loss: 0.9203 - val_acc: 0.6333\n",
      "Epoch 87/200\n",
      "120/120 [==============================] - 0s - loss: 0.1931 - acc: 0.9417 - val_loss: 0.9358 - val_acc: 0.6333\n",
      "Epoch 88/200\n",
      "120/120 [==============================] - 0s - loss: 0.1904 - acc: 0.9500 - val_loss: 0.9525 - val_acc: 0.6000\n",
      "Epoch 89/200\n",
      "120/120 [==============================] - 0s - loss: 0.1884 - acc: 0.9500 - val_loss: 0.9108 - val_acc: 0.6333\n",
      "Epoch 90/200\n",
      "120/120 [==============================] - 0s - loss: 0.1831 - acc: 0.9500 - val_loss: 0.8480 - val_acc: 0.7000\n",
      "Epoch 91/200\n",
      "120/120 [==============================] - 0s - loss: 0.1805 - acc: 0.9583 - val_loss: 0.8531 - val_acc: 0.7000\n",
      "Epoch 92/200\n",
      "120/120 [==============================] - 0s - loss: 0.1766 - acc: 0.9500 - val_loss: 0.9003 - val_acc: 0.6333\n",
      "Epoch 93/200\n",
      "120/120 [==============================] - 0s - loss: 0.1733 - acc: 0.9500 - val_loss: 0.7833 - val_acc: 0.7333\n",
      "Epoch 94/200\n",
      "120/120 [==============================] - 0s - loss: 0.1722 - acc: 0.9667 - val_loss: 0.8329 - val_acc: 0.7000\n",
      "Epoch 95/200\n",
      "120/120 [==============================] - 0s - loss: 0.1684 - acc: 0.9417 - val_loss: 0.7748 - val_acc: 0.7333\n",
      "Epoch 96/200\n",
      "120/120 [==============================] - 0s - loss: 0.1678 - acc: 0.9583 - val_loss: 0.8159 - val_acc: 0.7000\n",
      "Epoch 97/200\n",
      "120/120 [==============================] - 0s - loss: 0.1638 - acc: 0.9583 - val_loss: 0.7908 - val_acc: 0.7000\n",
      "Epoch 98/200\n",
      "120/120 [==============================] - 0s - loss: 0.1596 - acc: 0.9667 - val_loss: 0.8182 - val_acc: 0.7000\n",
      "Epoch 99/200\n",
      "120/120 [==============================] - 0s - loss: 0.1571 - acc: 0.9667 - val_loss: 0.8612 - val_acc: 0.6333\n",
      "Epoch 100/200\n",
      "120/120 [==============================] - 0s - loss: 0.1560 - acc: 0.9583 - val_loss: 0.8592 - val_acc: 0.6333\n",
      "Epoch 101/200\n",
      "120/120 [==============================] - 0s - loss: 0.1539 - acc: 0.9500 - val_loss: 0.8037 - val_acc: 0.7000\n",
      "Epoch 102/200\n",
      "120/120 [==============================] - 0s - loss: 0.1486 - acc: 0.9750 - val_loss: 0.7825 - val_acc: 0.7000\n",
      "Epoch 103/200\n",
      "120/120 [==============================] - 0s - loss: 0.1486 - acc: 0.9667 - val_loss: 0.8137 - val_acc: 0.7000\n",
      "Epoch 104/200\n",
      "120/120 [==============================] - 0s - loss: 0.1477 - acc: 0.9667 - val_loss: 0.7372 - val_acc: 0.7333\n",
      "Epoch 105/200\n",
      "120/120 [==============================] - 0s - loss: 0.1421 - acc: 0.9667 - val_loss: 0.7236 - val_acc: 0.7333\n",
      "Epoch 106/200\n",
      "120/120 [==============================] - 0s - loss: 0.1422 - acc: 0.9667 - val_loss: 0.7630 - val_acc: 0.7000\n",
      "Epoch 107/200\n",
      "120/120 [==============================] - 0s - loss: 0.1383 - acc: 0.9750 - val_loss: 0.7101 - val_acc: 0.7667\n",
      "Epoch 108/200\n",
      "120/120 [==============================] - 0s - loss: 0.1357 - acc: 0.9583 - val_loss: 0.6547 - val_acc: 0.8000\n",
      "Epoch 109/200\n",
      "120/120 [==============================] - 0s - loss: 0.1341 - acc: 0.9667 - val_loss: 0.7003 - val_acc: 0.7667\n",
      "Epoch 110/200\n",
      "120/120 [==============================] - 0s - loss: 0.1322 - acc: 0.9583 - val_loss: 0.7475 - val_acc: 0.7000\n",
      "Epoch 111/200\n",
      "120/120 [==============================] - 0s - loss: 0.1272 - acc: 0.9750 - val_loss: 0.8585 - val_acc: 0.6333\n",
      "Epoch 112/200\n",
      "120/120 [==============================] - 0s - loss: 0.1294 - acc: 0.9583 - val_loss: 0.6713 - val_acc: 0.8000\n",
      "Epoch 113/200\n",
      "120/120 [==============================] - 0s - loss: 0.1263 - acc: 0.9750 - val_loss: 0.6239 - val_acc: 0.8000\n",
      "Epoch 114/200\n",
      "120/120 [==============================] - 0s - loss: 0.1243 - acc: 0.9750 - val_loss: 0.7151 - val_acc: 0.7333\n",
      "Epoch 115/200\n",
      "120/120 [==============================] - 0s - loss: 0.1218 - acc: 0.9750 - val_loss: 0.7594 - val_acc: 0.7000\n",
      "Epoch 116/200\n",
      "120/120 [==============================] - 0s - loss: 0.1213 - acc: 0.9583 - val_loss: 0.7562 - val_acc: 0.7000\n",
      "Epoch 117/200\n",
      "120/120 [==============================] - 0s - loss: 0.1179 - acc: 0.9667 - val_loss: 0.5996 - val_acc: 0.8000\n",
      "Epoch 118/200\n",
      "120/120 [==============================] - 0s - loss: 0.1190 - acc: 0.9667 - val_loss: 0.6394 - val_acc: 0.8000\n",
      "Epoch 119/200\n",
      "120/120 [==============================] - 0s - loss: 0.1095 - acc: 0.9667 - val_loss: 0.8503 - val_acc: 0.6333\n",
      "Epoch 120/200\n",
      "120/120 [==============================] - 0s - loss: 0.1146 - acc: 0.9667 - val_loss: 0.5891 - val_acc: 0.8000\n",
      "Epoch 121/200\n",
      "120/120 [==============================] - 0s - loss: 0.1132 - acc: 0.9667 - val_loss: 0.6219 - val_acc: 0.8000\n",
      "Epoch 122/200\n",
      "120/120 [==============================] - 0s - loss: 0.1105 - acc: 0.9750 - val_loss: 0.5973 - val_acc: 0.8000\n",
      "Epoch 123/200\n",
      "120/120 [==============================] - 0s - loss: 0.1050 - acc: 0.9500 - val_loss: 0.7837 - val_acc: 0.7000\n",
      "Epoch 124/200\n",
      "120/120 [==============================] - 0s - loss: 0.1108 - acc: 0.9667 - val_loss: 0.6714 - val_acc: 0.8000\n",
      "Epoch 125/200\n",
      "120/120 [==============================] - 0s - loss: 0.1075 - acc: 0.9667 - val_loss: 0.6752 - val_acc: 0.8000\n",
      "Epoch 126/200\n",
      "120/120 [==============================] - 0s - loss: 0.1068 - acc: 0.9667 - val_loss: 0.6302 - val_acc: 0.8000\n",
      "Epoch 127/200\n",
      "120/120 [==============================] - 0s - loss: 0.1042 - acc: 0.9583 - val_loss: 0.6935 - val_acc: 0.7667\n",
      "Epoch 128/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s - loss: 0.1046 - acc: 0.9750 - val_loss: 0.5908 - val_acc: 0.8000\n",
      "Epoch 129/200\n",
      "120/120 [==============================] - 0s - loss: 0.1034 - acc: 0.9667 - val_loss: 0.6213 - val_acc: 0.8000\n",
      "Epoch 130/200\n",
      "120/120 [==============================] - 0s - loss: 0.1013 - acc: 0.9583 - val_loss: 0.5596 - val_acc: 0.8000\n",
      "Epoch 131/200\n",
      "120/120 [==============================] - 0s - loss: 0.0977 - acc: 0.9667 - val_loss: 0.6850 - val_acc: 0.8000\n",
      "Epoch 132/200\n",
      "120/120 [==============================] - 0s - loss: 0.0993 - acc: 0.9750 - val_loss: 0.6208 - val_acc: 0.8000\n",
      "Epoch 133/200\n",
      "120/120 [==============================] - 0s - loss: 0.0937 - acc: 0.9750 - val_loss: 0.7302 - val_acc: 0.7333\n",
      "Epoch 134/200\n",
      "120/120 [==============================] - 0s - loss: 0.0973 - acc: 0.9667 - val_loss: 0.6757 - val_acc: 0.8000\n",
      "Epoch 135/200\n",
      "120/120 [==============================] - 0s - loss: 0.0945 - acc: 0.9667 - val_loss: 0.5315 - val_acc: 0.8000\n",
      "Epoch 136/200\n",
      "120/120 [==============================] - 0s - loss: 0.0881 - acc: 0.9750 - val_loss: 0.7883 - val_acc: 0.7000\n",
      "Epoch 137/200\n",
      "120/120 [==============================] - 0s - loss: 0.0926 - acc: 0.9750 - val_loss: 0.5131 - val_acc: 0.8000\n",
      "Epoch 138/200\n",
      "120/120 [==============================] - 0s - loss: 0.0938 - acc: 0.9583 - val_loss: 0.5415 - val_acc: 0.8000\n",
      "Epoch 139/200\n",
      "120/120 [==============================] - 0s - loss: 0.0920 - acc: 0.9667 - val_loss: 0.5190 - val_acc: 0.8000\n",
      "Epoch 140/200\n",
      "120/120 [==============================] - 0s - loss: 0.0922 - acc: 0.9667 - val_loss: 0.5878 - val_acc: 0.8000\n",
      "Epoch 141/200\n",
      "120/120 [==============================] - 0s - loss: 0.0910 - acc: 0.9667 - val_loss: 0.5976 - val_acc: 0.8000\n",
      "Epoch 142/200\n",
      "120/120 [==============================] - 0s - loss: 0.0882 - acc: 0.9583 - val_loss: 0.6214 - val_acc: 0.8000\n",
      "Epoch 143/200\n",
      "120/120 [==============================] - 0s - loss: 0.0855 - acc: 0.9583 - val_loss: 0.7306 - val_acc: 0.7333\n",
      "Epoch 144/200\n",
      "120/120 [==============================] - 0s - loss: 0.0892 - acc: 0.9667 - val_loss: 0.5873 - val_acc: 0.8000\n",
      "Epoch 145/200\n",
      "120/120 [==============================] - 0s - loss: 0.0843 - acc: 0.9750 - val_loss: 0.5005 - val_acc: 0.8000\n",
      "Epoch 146/200\n",
      "120/120 [==============================] - 0s - loss: 0.0856 - acc: 0.9750 - val_loss: 0.6389 - val_acc: 0.8000\n",
      "Epoch 147/200\n",
      "120/120 [==============================] - 0s - loss: 0.0854 - acc: 0.9667 - val_loss: 0.6723 - val_acc: 0.8000\n",
      "Epoch 148/200\n",
      "120/120 [==============================] - 0s - loss: 0.0840 - acc: 0.9667 - val_loss: 0.5991 - val_acc: 0.8000\n",
      "Epoch 149/200\n",
      "120/120 [==============================] - 0s - loss: 0.0827 - acc: 0.9583 - val_loss: 0.6615 - val_acc: 0.8000\n",
      "Epoch 150/200\n",
      "120/120 [==============================] - 0s - loss: 0.0830 - acc: 0.9667 - val_loss: 0.6158 - val_acc: 0.8000\n",
      "Epoch 151/200\n",
      "120/120 [==============================] - 0s - loss: 0.0827 - acc: 0.9667 - val_loss: 0.5906 - val_acc: 0.8000\n",
      "Epoch 152/200\n",
      "120/120 [==============================] - 0s - loss: 0.0820 - acc: 0.9667 - val_loss: 0.6170 - val_acc: 0.8000\n",
      "Epoch 153/200\n",
      "120/120 [==============================] - 0s - loss: 0.0752 - acc: 0.9667 - val_loss: 0.7479 - val_acc: 0.7333\n",
      "Epoch 154/200\n",
      "120/120 [==============================] - 0s - loss: 0.0841 - acc: 0.9750 - val_loss: 0.6747 - val_acc: 0.8000\n",
      "Epoch 155/200\n",
      "120/120 [==============================] - 0s - loss: 0.0780 - acc: 0.9667 - val_loss: 0.5597 - val_acc: 0.8000\n",
      "Epoch 156/200\n",
      "120/120 [==============================] - 0s - loss: 0.0805 - acc: 0.9667 - val_loss: 0.5393 - val_acc: 0.8000\n",
      "Epoch 157/200\n",
      "120/120 [==============================] - 0s - loss: 0.0773 - acc: 0.9667 - val_loss: 0.5729 - val_acc: 0.8000\n",
      "Epoch 158/200\n",
      "120/120 [==============================] - 0s - loss: 0.0762 - acc: 0.9583 - val_loss: 0.6382 - val_acc: 0.8000\n",
      "Epoch 159/200\n",
      "120/120 [==============================] - 0s - loss: 0.0773 - acc: 0.9667 - val_loss: 0.5573 - val_acc: 0.8000\n",
      "Epoch 160/200\n",
      "120/120 [==============================] - 0s - loss: 0.0751 - acc: 0.9667 - val_loss: 0.4869 - val_acc: 0.8333\n",
      "Epoch 161/200\n",
      "120/120 [==============================] - 0s - loss: 0.0771 - acc: 0.9667 - val_loss: 0.5800 - val_acc: 0.8000\n",
      "Epoch 162/200\n",
      "120/120 [==============================] - 0s - loss: 0.0744 - acc: 0.9667 - val_loss: 0.5379 - val_acc: 0.8000\n",
      "Epoch 163/200\n",
      "120/120 [==============================] - 0s - loss: 0.0730 - acc: 0.9667 - val_loss: 0.5654 - val_acc: 0.8000\n",
      "Epoch 164/200\n",
      "120/120 [==============================] - 0s - loss: 0.0734 - acc: 0.9750 - val_loss: 0.6784 - val_acc: 0.8000\n",
      "Epoch 165/200\n",
      "120/120 [==============================] - 0s - loss: 0.0734 - acc: 0.9750 - val_loss: 0.6283 - val_acc: 0.8000\n",
      "Epoch 166/200\n",
      "120/120 [==============================] - 0s - loss: 0.0700 - acc: 0.9667 - val_loss: 0.6775 - val_acc: 0.8000\n",
      "Epoch 167/200\n",
      "120/120 [==============================] - 0s - loss: 0.0688 - acc: 0.9667 - val_loss: 0.7128 - val_acc: 0.7667\n",
      "Epoch 168/200\n",
      "120/120 [==============================] - 0s - loss: 0.0719 - acc: 0.9667 - val_loss: 0.5350 - val_acc: 0.8000\n",
      "Epoch 169/200\n",
      "120/120 [==============================] - 0s - loss: 0.0710 - acc: 0.9667 - val_loss: 0.5487 - val_acc: 0.8000\n",
      "Epoch 170/200\n",
      "120/120 [==============================] - 0s - loss: 0.0691 - acc: 0.9667 - val_loss: 0.5271 - val_acc: 0.8000\n",
      "Epoch 171/200\n",
      "120/120 [==============================] - 0s - loss: 0.0723 - acc: 0.9667 - val_loss: 0.5725 - val_acc: 0.8000\n",
      "Epoch 172/200\n",
      "120/120 [==============================] - 0s - loss: 0.0690 - acc: 0.9667 - val_loss: 0.6000 - val_acc: 0.8000\n",
      "Epoch 173/200\n",
      "120/120 [==============================] - 0s - loss: 0.0688 - acc: 0.9667 - val_loss: 0.5217 - val_acc: 0.8333\n",
      "Epoch 174/200\n",
      "120/120 [==============================] - 0s - loss: 0.0683 - acc: 0.9667 - val_loss: 0.5601 - val_acc: 0.8000\n",
      "Epoch 175/200\n",
      "120/120 [==============================] - 0s - loss: 0.0708 - acc: 0.9667 - val_loss: 0.5697 - val_acc: 0.8000\n",
      "Epoch 176/200\n",
      "120/120 [==============================] - 0s - loss: 0.0666 - acc: 0.9667 - val_loss: 0.4863 - val_acc: 0.8333\n",
      "Epoch 177/200\n",
      "120/120 [==============================] - 0s - loss: 0.0616 - acc: 0.9750 - val_loss: 0.7351 - val_acc: 0.7333\n",
      "Epoch 178/200\n",
      "120/120 [==============================] - 0s - loss: 0.0677 - acc: 0.9667 - val_loss: 0.5447 - val_acc: 0.8000\n",
      "Epoch 179/200\n",
      "120/120 [==============================] - 0s - loss: 0.0639 - acc: 0.9833 - val_loss: 0.6775 - val_acc: 0.8000\n",
      "Epoch 180/200\n",
      "120/120 [==============================] - 0s - loss: 0.0666 - acc: 0.9667 - val_loss: 0.7084 - val_acc: 0.8000\n",
      "Epoch 181/200\n",
      "120/120 [==============================] - 0s - loss: 0.0673 - acc: 0.9667 - val_loss: 0.5963 - val_acc: 0.8000\n",
      "Epoch 182/200\n",
      "120/120 [==============================] - 0s - loss: 0.0640 - acc: 0.9833 - val_loss: 0.5491 - val_acc: 0.8333\n",
      "Epoch 183/200\n",
      "120/120 [==============================] - 0s - loss: 0.0668 - acc: 0.9667 - val_loss: 0.5633 - val_acc: 0.8000\n",
      "Epoch 184/200\n",
      "120/120 [==============================] - 0s - loss: 0.0606 - acc: 0.9750 - val_loss: 0.7595 - val_acc: 0.7333\n",
      "Epoch 185/200\n",
      "120/120 [==============================] - 0s - loss: 0.0642 - acc: 0.9750 - val_loss: 0.5684 - val_acc: 0.8000\n",
      "Epoch 186/200\n",
      "120/120 [==============================] - 0s - loss: 0.0629 - acc: 0.9667 - val_loss: 0.5256 - val_acc: 0.8333\n",
      "Epoch 187/200\n",
      "120/120 [==============================] - 0s - loss: 0.0628 - acc: 0.9750 - val_loss: 0.6359 - val_acc: 0.8000\n",
      "Epoch 188/200\n",
      "120/120 [==============================] - 0s - loss: 0.0612 - acc: 0.9667 - val_loss: 0.6104 - val_acc: 0.8000\n",
      "Epoch 189/200\n",
      "120/120 [==============================] - 0s - loss: 0.0623 - acc: 0.9750 - val_loss: 0.5873 - val_acc: 0.8000\n",
      "Epoch 190/200\n",
      "120/120 [==============================] - 0s - loss: 0.0622 - acc: 0.9667 - val_loss: 0.5739 - val_acc: 0.8333\n",
      "Epoch 191/200\n",
      "120/120 [==============================] - 0s - loss: 0.0608 - acc: 0.9667 - val_loss: 0.5347 - val_acc: 0.8333\n",
      "Epoch 192/200\n",
      "120/120 [==============================] - 0s - loss: 0.0609 - acc: 0.9667 - val_loss: 0.5262 - val_acc: 0.8333\n",
      "Epoch 193/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s - loss: 0.0579 - acc: 0.9833 - val_loss: 0.5626 - val_acc: 0.8333\n",
      "Epoch 194/200\n",
      "120/120 [==============================] - 0s - loss: 0.0601 - acc: 0.9750 - val_loss: 0.5240 - val_acc: 0.8333\n",
      "Epoch 195/200\n",
      "120/120 [==============================] - 0s - loss: 0.0595 - acc: 0.9667 - val_loss: 0.6036 - val_acc: 0.8000\n",
      "Epoch 196/200\n",
      "120/120 [==============================] - 0s - loss: 0.0607 - acc: 0.9667 - val_loss: 0.6424 - val_acc: 0.8000\n",
      "Epoch 197/200\n",
      "120/120 [==============================] - 0s - loss: 0.0586 - acc: 0.9583 - val_loss: 0.5598 - val_acc: 0.8333\n",
      "Epoch 198/200\n",
      "120/120 [==============================] - 0s - loss: 0.0546 - acc: 0.9750 - val_loss: 0.6185 - val_acc: 0.8000\n",
      "Epoch 199/200\n",
      "120/120 [==============================] - 0s - loss: 0.0569 - acc: 0.9667 - val_loss: 0.5339 - val_acc: 0.8333\n",
      "Epoch 200/200\n",
      "120/120 [==============================] - 0s - loss: 0.0581 - acc: 0.9750 - val_loss: 0.5632 - val_acc: 0.8333\n"
     ]
    }
   ],
   "source": [
    "history = model_v.fit(X,y,epochs=200,batch_size=5,validation_split=0.2,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 94.67%\n"
     ]
    }
   ],
   "source": [
    "scores = model_v.evaluate(X,y,verbose=0)\n",
    "print('%s: %.2f%%' % (model_v.metrics_names[1],scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    }
   ],
   "source": [
    "# History列表\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 每一次的迭代（epoch）有一个结果\n",
    "len(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy trend')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['train','validation'],loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss trend')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train','validation'],loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
